\section{Introduction}

The modern data analyst has no shortage of clustering algorithms to choose from but, given the ever-increasing size of relevant datasets, many are often
too slow to be practically useful. 
Indeed, Llyod's method \cite{Lloyd82}, the arguably most popular used clustering algorithm, runs for multiple iterations until convergence with every iteration requiring $O(ndk)$ time, where $n$ is the number of points, $d$ is the number of features and $k$ is the number of clusters. This is prohibitive whenever the data set is very large and has prompted the rise of big data algorithms which provide both theoretical guarantees and
practical improvements for standard data-science techniques. The perspectives
of theoretical soundness and practical efficacy can, however, sometimes be at odds with one another. On the one hand, theoretical guarantees provide assurance that
the algorithm will obtain valid solutions without unlucky inputs affecting their running time or accuracy. On the other hand, it is sometimes difficult to convince
oneself to implement the theoretically optimal algorithm when there are cruder methods that are faster to get running and perform well in practice.

Since datasets can be large in the number of points $n$ and/or the number of features $d$, big-data methods must mitigate the effects of both.
With respect to the feature space, the question is effectively closed as random projections are fast (running in effectively linear time), practical to
implement \cite{makarychev2019performance}, and provide tight guarantees on the embedding's size and quality. The outlook is less clear when reducing the number of points $n$, and there are
two separate paradigms that each provide distinct advantages. 
On the one hand, we have uniform sampling, which runs in sublinear time but may miss important subsets of
the data and therefore can only guarantee accuracy when making assumptions on the data \cite{HuangJL23}.  On the other hand, the most accurate sampling strategies are those that provide the \emph{strong coreset}
guarantee, wherein the cost of any solution on the compressed data is within an $\varepsilon$-factor of that solution's cost on the original dataset \cite{CSS21}.
Surprisingly, many coreset constructions can remove the dependency on $n$, significantly accelerating downstream tasks.

We study both paradigms with respect to a classic problem -- what are the limits and possibilities of compression for the $k$-means and $k$-median objectives?
Whereas uniform sampling provides optimal speed but no worst case accuracy guarantee, all claimed coreset constructions have a running time of at least $\tilde{O}(nd+nk)$ when yielding tight bounds on the
minimum number of samples required for accurate compression. 

It is easy to show that any algorithm that achieves a guarantee must read the entire data\footnote{A simple example is to consider a data matrix with one single large entry and the remaining entries being very small. The algorithm must find this entry to yield a good 2-clustering.}. Thus a clear open question is what guarantees are achievable in linear or nearly linear time.
\chris{I don't like the following sentence. It sounds negative. There is little to study, so why are we doing it? I wrote an alternative before.}
Although there is little to study with respect to random sampling, the lack of clarity regarding
linear-time coreset constructions throws into question their ability to facilitate clustering on large datasets. Indeed, currently available fast sampling
algorithms for clustering \cite{BachemL018} \cite{kmeans_sublinear_bachem16} cannot achieve strong coreset guarantees.  Recently, \cite{DSWY22} proposed
a sensitivity-based method for strong coresets that runs in time $\tilde{O}(nd + nk)$ and conjectured this to be optimal for $k$-median and $k$-means.  Since
the issue of determining an optimal coreset size has recently been resolved \cite{CSS21,CLSSS22,HLW23}, this is arguably the main open problem in coreset
research for center-based clustering. We resolve this by showing that there exists an easy-to-implement algorithm that constructs coresets in $\tilde{O}(nd)$
time -- only logarithmic factors away from the time it takes to read in the dataset.

Nonetheless, this does not fully illuminate the landscape between the sampling paradigms for clustering algorithms when applying them in practice. Although our
algorithm achieves an optimal running time and an optimal compression, it is certainly possible that other, cruder methods may be just as viable on most real world data sets despite
having no worst case guarantees.  Thus, there is a natural spectrum within linear-time algorithms where obtaining more information about the dataset allows for
a better sampling strategy at the expense of sublinear factors in speed. We state this formally in the following question: When are optimal $k$-means and
$k$-median coresets necessary and what is the practical tradeoff between coreset speed and accuracy? To this end, we show that while many practical settings do
not require the full coreset guarantee, one cannot cut corners if one wants to be confident in their compression. We also show that this extends to the
streaming paradigm and applies to downstream clustering approaches.

In summary, our contributions are as follows:
\begin{itemize}
    \item Theoretical Contributions
    \begin{itemize}
        \item We show that one can obtain strong coresets for $k$-means and $k$-median in $\tilde{O}(nd \log \Delta)$ time\footnote{$\Delta$ is the ratio of
            maximum distance over minimum distance in the dataset.}.  This resolves a conjecture that the runtime requires an $\tilde{O}(nk)$ term \cite{DSWY22}. \chris{Why can't we just state the second result? Is there any reason for stating the first one?}
        \item We extend this by noting that the linear dependency on $\log \Delta$ can be avoided by appropriately reorganizing the space. This gives the first
            effectively optimal\footnote{up to log factors} $\tilde{O}(nd)$ time algorithm for $k$-means and $k$-median coresets.
    \end{itemize}
    \item Experimental Contributions
    \begin{itemize}
        \item We perform a comprehensive analysis of sampling methods for the $k$-means and $k$-median tasks, measuring runtime and compression quality in the streaming and non-streaming settings.
        \item We provide an intuitive blueprint for the practitioner on what tradeoff can be expected between speed and accuracy when clustering big-data. \chris{Can we be more concrete here? Let's try to give the reviewer the phrase we want them to write. Something like "we show that while uniform sampling is a very accurate algorithm for most data sets, it is not unconditionally viable. Among sampling strategies with gaurantees, we provide a comprehensive overview of tradeoffs between running time and accuracy. In essentially all cases, our optimal fast coreset algorithm achieves the best guarantees.}
    \end{itemize}
\end{itemize}
