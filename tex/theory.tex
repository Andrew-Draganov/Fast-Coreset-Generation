\section{Coreset Algorithms}
\label{sec:theory}

In this section, we first combine two existing results to produce a strong coreset in time $\tilde{O}(nd \log \Delta)$, where $\Delta$ is the aspect-ratio of
the input.  We show afterwards how to reduce the dependency in $\Delta$ to $\log \log \Delta$, giving the desired nearly-linear runtime.

Our method is based on the following observations about the group sampling \cite{stoc21} and sensitivity sampling \cite{FeldmanL11} coreset construction
algorithms. Both start by computing a solution $\calC$. When $\calC$ is a $c$-approximation, they compute a $c \eps$-coreset of size $\tilde O\lpar
k \eps^{-z-2}\rpar$ and $\tilde O\lpar k \eps^{-2z -2}\rpar$, respectively. Hence, by rescaling, they provide $\eps$-coreset with size $\tilde O\lpar
k (\eps/c)^{-z-2}\rpar$ and $\tilde O\lpar k (\eps/c)^{-2z-2}\rpar$. 

This leads to the following fact:
\begin{fact}\label{fact:logApprox}
Let $\calC$ be an $O\lpar \log^{O(1)} k\rpar$ approximation to $k$-median or $k$-means.
Then, group sampling using solution $\calC$ computes a coreset of size $\tilde O\lpar
k \eps^{-z-2}\rpar$, and sensitivity sampling one of size $\tilde O\lpar k \eps^{-2z-2}\rpar$. 
Both runs in time $\tilde O(nd)$, provided $\calC$.
\end{fact}

To turn \cref{fact:logApprox} into an algorithm, we use the \fkmeans approximation algorithm from \cite{cohen2020fast}, which has the two following key properties: 
\begin{itemize}
\item \fkmeans runs in $\tilde O\lpar n d \log \Delta\rpar$ time (Corollary 4.3 in \cite{cohen2020fast}), and
\item \fkmeans computes an assignment from input point to centers that is a $O\lpar d^z \log k\rpar$ approximation to $k$-median ($z=1$) and
$k$-means ($z=2$) (Lemma 3.1 in \cite{cohen2020fast} for $z=2$, the discussion above for $z=1$). Applying dimension reduction techniques \cite{MakarychevMR19}, the dimension $d$ may be replaced by a $\log k$. This results in a $O\lpar\log^{z+1} k\rpar$ approximation.
\end{itemize}

The second property is crucial for us: the algorithm does not only compute centers, but also assignments in $\tilde{O}(nd\log \Delta)$ time.  We use it, in
combination with sensitivity sampling, as described in \cref{alg:main}.  This algorithm computes an $\eps$-coreset in time $\tilde O(nd \log \Delta)$: we prove
formally the statement in \cref{app:theory}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrrr}
        \hline
        Value of $r$ & 20 & 30 & 40 & 50 \\
        Runtime (s) & 13.5 & 14.3 & 15.7 & 16.1 \\
        \hline
        \vspace*{0.1cm}
    \end{tabular}
    \caption{Mean runtime in seconds of the \fkmeans algorithm as a function of $r$ on the dataset described at the end of Section \ref{sec:theory}. The mean is
    taken over 5 runs.}
    \label{tbl:logdelta}
\end{table}

Thus, it is easy to combine existing results to obtain an $\eps$-coreset without an $\tilde{O}(nk)$ time-dependency.  However, our method thus far has only
replaced the $\tilde{O}(nd + nk)$ runtime by $\tilde{O}(nd \log \Delta)$. Indeed, the spirit of the issue remains -- this is not log-linear in the input size.
We verify this by devising a dataset that has $n - n'$ points uniformly in the $[-1, 1]^2$ square. Then, for $r \in \mathbb{Z}^+$, we produce a sequence of
points at $(0, 1), (0, 0.5), \cdots, (0, 0.5^r)$ and copy this sequence $n' / r$ times, each time with a different $x$ coordinate. The result is a dataset of
size $n$ where a quadtree embedding always has constant breadth but the branch depths (and correspondingly $\log \Delta$) grow linearly with $r$. The results
can be seen in Table \ref{tbl:logdelta}, where the runtime grows linearly with the ratio of the maximum and minimum distance.

\begin{algorithm}[tb]
   \caption{Fast-Coreset($P, k, \eps, S$)}
   \label{alg:main}
\begin{algorithmic}[1]
   \State {\bfseries Input:} data $P$, number of clusters $k$, precision $\eps$ and target size $S$
   \State Use a Fast Johnson-Lindenstrauss \andrew{refs?} embedding to compute the embedding $\tilde P$ of $P$ into $\tilde d = O(\log k)$ dimensions
   \State Compute an approximate solution $\tilde \calC = \lbra \tilde c_1, ..., \tilde c_k\rbra $ for $\tilde P$, and an assignment $\tilde \sigma : \tilde P \rightarrow \tilde \calC$ using \fkmeans.	
   \State Let $\calC_i = \tilde \sigma^{-1}(c_i)$. Compute the $1$-median (or $1$-mean) $c_i$ of each $\calC_i$ in $\R^d$.%, and define $\sigma(p) := c_i$ for all $p \in \calC_i$.
   \State For each point $p \in \calC_i$, define
   $s(p) = \frac{\dist^z(p, c_i)}{\cost(\calC_i, c_i)}+ \frac{1}{|\calC_i|}$.
   \State Compute a set $\coreset$ of $S$ points randomly sampled from $P$ proportionate to $s$.
   \State For each $\calC_i$, define $|\hat \calC_i|$ the estimated weight of $\calC_i$ by $\coreset$, namely $|\hat \calC_i| := \sum_{p \in \calC_i \cap \coreset} \frac{\sum_{p' \in P}s(p')}{s(p)S}$.
   \State {\bfseries Output:} the coreset $\coreset$, with weights $w(p) = \frac{\sum_{p' \in P}s(p')}{s(p)S} \lpar (1+\eps)|\calC_i| - |\hat \calC_i|\rpar$
\end{algorithmic}
\end{algorithm}

\section{Reducing the Impact of the Spread}
\label{sec:logdelta}
\newcommand{\boxsize}{\textsc{MaxDist}}

\subsection{Overview of the Approach}

To remove the $\log\Delta$ dependency, we proceed in two steps: first, we compute a very crude upper-bound on the cost $U$ of the optimal solution -- up to
a $\poly(n)$ factor.  If $U$ is a $c$-approximation of the optimal cost, the natural strategy to reduce the aspect-ratio is to round all coordinates to
multiples of $g = U/(cn)$, giving us a minimum distance of no less than $g$; it is then enough to reduce the diameter to $\poly(n) U$.  Thus, we place a grid
with cell length $O(n \cdot U)$, so that two points from the same cluster in $\opt$ fall into the same cell w.h.p., implying that distinct cells do not interact
with each other in any reasonable solution.  Then, we compress the input by "moving" non-empty cells closer to each other.

We focus here on the simpler $k$-median problem\footnote{\cref{app:redKM} shows how to reduce $k$-means to this case} and a description of the tree
construction can be found in \cref{app:quadtree}.

Given a quadtree embedding, any solution $\calS$'s cost in the tree metric is within a factor $O(\sqrt d \log \Delta)$ of its cost for the original metric.
Therefore, it is enough for us to find an approximation in the tree and we use $\opt_T$ to refer to the optimal solution in the quadtree.  We now show that (1)
we only need to build a fraction of the tree, allowing for a running time $O(nd \log \log \Delta)$, and (2) once the tree $T$ is built, there is a corresponding
tree $T'$ that has a similar optimal $k$-median cost but where $\Delta \in \poly(n, d)$.

\subsection{Computing a crude upper-bound}

As described, we start by computing an approximate solution $U$ such that $U \leq \poly(n) \cdot \cost(\opt)$ on a tree embedded dataset.  Our first lemma shows
that the necessary approximation exists at the first level for which the input lies in $k+1$ disjoint subtrees. The algorithm to find this is then a simple binary
search through the levels of the tree.

\begin{lemma}\label{lem:apxTree}
Let $i$ be the first level of the decomposition such that at least $k+1$ cells at level $i$ contains any point. Then, $\sqrt{d}2^{-i+1} \cdot \boxsize \leq
\opt_T \leq n \cdot \sqrt{d}2^{-i+4} \cdot \boxsize$.
\end{lemma}

We prove this in Section \ref{app:apx-tree-proof} of the appendix. A direct consequence of this lemma is that the first level of the tree for which at least
$k+1$ cells are non empty provides an $O(n)$-approximation. To count the number of non-empty cells at a given level $i$, one can merely iterate over all
points, for each point, identify the cell that contains it (using a modulo operation), and store all those cells into a hash table to count the number of elements.
This is done in time $O(nd)$.  Using a binary search on the $O(\log \Delta)$ many levels then concludes this section with the following result:

\begin{lemma}\label{lem:crudeApx}
There is an algorithm running in time $O(nd \log \log \Delta)$ that computes an  $O(n^2 d \log \Delta)$-approximation to $k$-median or $k$-means.
\end{lemma}

\subsection{From Approximate Solution to Small Aspect-Ratio}
Let $U$ be an upper-bound on the optimal cost that is a $c$-approximation, computed via \cref{lem:crudeApx}. We place a grid with side length $d n^2\cdot U$.
The following folklore lemma ensures that with high probability, no cluster of the optimal solution is spread on several grid cells.

\begin{lemma}
The probability that two points $p$ and $q$ are in different grid cells is $O\lpar \frac{\|p-q\|^2}{n^2 U}\rpar$
\end{lemma}

Since $U$ is larger than the distance between any input point and its center in the optimal solution, a union-bound ensures that with probability $1-1/n$, no
cluster of this solution is split among different cells.  In particular, there are at most $k$-non empty cells. We call those "boxes".

From this input, we build a new set of points $P'$ as follows: first, identify the non empty cell (using a hash table as previously). We associate each box with
its center.  For each coordinate $i \in \lbra 1, ..., d \rbra$, sort (in time $O(k \log k)$) the centers according to their value on coordinate $i$. Then, for
each $j \in \lbra 1, ..., k\rbra$, let $c^i_j$ and $c^i_{j+1}$ be the $i$-th coordinate of centers of the $j$-th and $(j+1)$-th boxes. If $c^i_{j+1} - c^i_j
\geq 2d n^2\cdot U$, then for all cells $j'$ with $j' > j$, shift the points of $j'$ by $c^i_{j+1} - c^i_j - 2d n^2\cdot U$ in the $i$-th coordinate.

This can be implemented with a linear scan and has two effects: first, the diameter of the input is now $\sqrt{d} \cdot 2d n^2\cdot U \cdot k$, as along any
coordinate the maximal distance is $2d n^2\cdot U \cdot k$. Second, two boxes that were adjacent are still adjacent and two boxes that were non-adjacent are
still non-adjacent.

The first property allows us to reduce the aspect-ratio to $(nd \log \Delta)^{O(1)}$.  Indeed, one can round all coordinates to the closest multiple of
$g = \frac{U}{n^4 d^{2} \log \Delta}$. Since every point has moved by at most $g$ and, using \cref{lem:crudeApx}, $U
\leq n^2 d \log(\Delta) \opt$, it is clear that the distance between any point and its rounding is at most $\frac{\opt}{n^2}$. Summing this error over all points,
any solution computed on the gridded data has cost within an additive factor $\pm \frac{\opt}{n}$ of the true cost. Furthermore, the smallest
non-zero distance is $g = \frac{U}{n^4 d^{2} \log \Delta}$, implying that the aspect-ratio of the new metric is $(nd \log \Delta)^{O(1)}$,
as claimed.

The second property, on the other hand, ensures that we can transform a solution $\calS'$ for $P'$ to a solution with exactly the same cost for $P$: in any
(reasonable) solution, points from two non-adjacent boxes will not be in the same cluster in either $P'$ or $P$. Therefore, simply adding back the corresponding
shift to centers of $\calS'$ allows us to transform it to a solution $\calS$. We note that the distance between any point and its closest center does not
change. This is formalized in the next lemma.

\begin{lemma}
Let $\calS'$ be a $c'$-approximation for  $k$-median (resp. $k$-means) on $P'$, where $c' \leq nc$ and $c$ is the approximation guarantee from
\cref{lem:crudeApx}. Then, one can compute a solution for $P$ for $k$-median (resp. $k$-means) on $P$, with same cost as $\calS'$ for $P'$, in time $O(nd)$.
\end{lemma}
\begin{proof}

First, since distances in $P'$ are smaller than in $P$, the optimal solution for $P'$ has cost at most $U$. Therefore, two points that are in non-adjacent boxes
(i.e., at distance more than $d n^2\cdot U$) are not in the same cluster of $\calS'$ -- as otherwise $\calS'$ would not be a $c'$-approximation.  Let $\calS$ be
the solution obtained from $\calS'$ by reversing the construction of $P'$. Since this construction preserves adjacency, for all clusters of the solution, all
distances are the same in $P$ and $P'$. Therefore, the costs are equal.

\end{proof}

\subsection*{Extensions.} 
We conclude this section with a few remarks that allow us to generalize \cref{alg:main}.

Consider that \cref{alg:main}  only needs to be provided with an assignment to a solution that is a $O(\polylog k)$ approximationone, implying that one could
compute the solution $\calC$ via any algorithm that satisfies this.  This initial solution may as well be an $O\lpar\polylog \lnor P \rnor_0 \rpar$
approximation: using the iterative coreset construction from \cite{BravermanJKW21}, one could then derive a near-optimal coreset size, only suffering
a $O(\log^* n)$ loss in the running time.

As an example, we illustrate a different approach for $k$-median. One could first embed the input into a hierarchically separated tree (HST) with expected
distortion $O(\log \lnor P \rnor_0)$ \cite{FakcharoenpholRT03}. On such tree metrics, solving $k$-median can be done in linear time using dedicated algorithms
(see e.g. \cite{Cohen-AddadLNSS21}). Using the solution from the HST metric, one can compute a coreset, and iterate using the previous argument.  This embedding
into HST is very similar to what is done by the \fkmeans algorithm, but can be actually performed in \emph{any} metric space, not only Euclidean.  For instance,
in a metric described by a graph with $m$ edges, the running time of this construction would be near linear-time $\tilde O(m)$.
