\section{The algorithm, and proof of \cref{thm:main}}

In this section, we present our algorithm and a clean proof of \cref{thm:main}. Those are based on the following observations about the group
sampling \cite{stoc21} and sensitivity sampling \cite{FeldmanL11} coreset construction algorithms.

Both start by computing a solution $\calC$. When $\calC$ is a $c$-approximation, the two algorithms compute a $c \eps$-coreset of size $\tilde O\lpar
k \eps^{-z-2}\rpar$ and $\tilde O\lpar k \eps^{-2z -2}\rpar$, respectively. Hence, by rescaling, they provide $\eps$-coreset with size $\tilde O\lpar
k (\eps/c)^{-z-2}\rpar$ and $\tilde O\lpar k (\eps/c)^{-2z-2}\rpar$. 
%\footnote{For group sampling, this result is not formally stated, but can be observed in Definition 1 and Lemma 12 of \cite{stoc21}: if $\calC$ is a $c$ approximation, rescaling $\eps$ by $c$ provides an approximate centroid set of the right precision, and the right error on the coreset. For sensitivity sampling, this can be seen in Lemma 15.4 of \cite{FLArxiv}, where $B$ is our $\calC$: if it is a $c$ approximation, rescaling $\eps$ by $c$ ensures an error $\eps \opt$ in the last line of the proof.} 

This leads to the following fact:
\begin{fact}\label{fact:logApprox}
Let $\calC$ be an $O\lpar \log^{O(1)} k\rpar$ approximation to $k$-median or $k$-means.
Then, group sampling using solution $\calC$ computes a coreset of size $\tilde O\lpar
k \eps^{-z-2}\rpar$, and sensitivity sampling one of size $\tilde O\lpar k \eps^{-2z-2}\rpar$. 
Both runs in time $\tilde O(nd)$, provided $\calC$.
\end{fact}
%Therefore, taking $c = O(\log k)$ merely affects the size of the computed coreset, while allowing for approximation algorithms that are way faster.

%\david{remove the next two paragraphs?}
%Due to the difficulty of verifying coresets, it is important to have a theoretical proof of efficiency of the coreset algorithms.  Showing, as we do, that one can use $O(\log k)$-approximations (e.g., $k$-means++) while benefiting from the same
%guarantees as with a slow $O(1)$-approximation is thus not only an artifact, but a crucial piece of coreset's success. 
%
%It is indeed tempting to use those large-factor approximations, especially since they seem to perform better in practice. 
%It might however have been the case that coresets constructed that way have poor quality, without being able to notice it due to the aforementioned hardness. 
%We show that this unfortunate event does not happen, and that, even with the ``worst-case" guarantee $O(\log k)$, these algorithms indeed produce coresets.

Our algorithm is based on the \fkmeans algorithm from \cite{cohen2020fast}: it allows to embed points in a very simple metric,  where it is easy to compute the
distance to any given set of centers. The two key properties of this algorithm are the following: 
\begin{itemize}
\item \fkmeans runs in $\tilde O\lpar n d \log \Delta\rpar$ time (Corollary 4.3 in \cite{cohen2020fast}, and
\item \fkmeans computes an assignment from input point to centers that is a $O\lpar d^z \log k\rpar$ approximation to $k$-median ($z=1$) and
$k$-means ($z=2$) (Lemma 3.1 in \cite{cohen2020fast} for $z=2$, discussion above for $z=1$). Applying dimension reduction techniques \cite{MakarychevMR19}, the dependency on $d$ may be replaced by a dependency on $\log k$. This results in a $O\lpar\log^{z+1} k\rpar$ approximation.
\end{itemize}

The second property is crucial for us: the algorithm does not only compute centers, but also assignments. 
  We use it, in combination with sensitivity sampling, as described in \cref{alg:main}.


\begin{algorithm}[tb]
   \caption{Fast-Coreset}
   \label{alg:main}
\begin{algorithmic}[1]
   \State {\bfseries Input:} data $P$, number of clusters $k$, precision $\eps$ and target size $S$
   \State Use Johnson-Lindenstrauss embedding to compute the embedding $\tilde P$ of $P$ into $\tilde d = O(\log k)$ dimensions
   \State Compute an approximate solution $\tilde \calC = \lbra \tilde c_1, ..., \tilde c_k\rbra $ for $\tilde P$, and an assignment $\tilde \sigma : \tilde P \rightarrow \tilde \calC$ using \fkmeans.	
   \State Let $\calC_i = \tilde \sigma^{-1}(c_i)$.
   \State Compute the $1$-median (or $1$-mean) $c_i$ of each $\calC_i$ in $\R^d$.%, and define $\sigma(p) := c_i$ for all $p \in \calC_i$.
   \State For each point $p \in \calC_i$, define
   $s(p) = \frac{\dist^z(p, c_i)}{\cost(\calC_i, c_i)}+ \frac{1}{|\calC_i|}$.
   \State Compute a set $\coreset$ of $S$ points randomly sampled from $P$ proportionate to $s$.
   \State For each $\calC_i$, define $|\hat \calC_i|$ the estimated weight of $\calC_i$ by $\coreset$, namely $|\hat \calC_i| := \sum_{p \in \calC_i \cap \coreset} \frac{\sum_{p' \in P}s(p')}{s(p)S}$.
   \State {\bfseries Output:} the coreset $\coreset$, with weights $w(p) = \frac{\sum_{p' \in P}s(p')}{s(p)S} \lpar (1+\eps)|\calC_i| - |\hat \calC_i|\rpar$
\end{algorithmic}
\end{algorithm}


Given this algorithm, we formalize the proof of \cref{thm:main}.

\begin{proof}[Proof of \cref{thm:main}]
We show that \cref{alg:main} has the desired guarantees.
First, performing the Johnson-Lindenstrauss embedding takes time $\tilde O(nd)$.

On the projected dataset, the algorithm \fkmeans runs in time $\tilde O\lpar n \log \Delta\rpar$, and the solution it has an approximation-ratio $O\lpar \tilde{d}^z \log k\rpar = O\lpar\log^{z+1} k\rpar$ for $\tilde P$. 
The guarantee offered by the embedding ensure that the clustering $\lbra \calC_1,...,\calC_k\rbra$ still has approximation ratio for $P$ \cite{makarychev2019performance}. 

For $k$-means, computing the $1$-mean solution for each $\calC_i$ takes time $O(nd)$ (the $1$-mean is simply the mean). 
For $k$-median, computing the $1$-median solution can be done as well in time $O(nd)$ \cite{CohenLMPS16}. 
We note that both may be approximated to a factor $2$ in constant time, by sampling uniformly at randm few points from each cluster \cite{neurips21}.

Provided the $c_i$ and the partition $\calC_i$, computing $|\calC_i|$ and $\cost(\calC_i, c_i)$ for all $i$ also takes time $O(nd)$.

Since the solution consisting of assigning each $p \in \calC_i$ to $c_i$ is a $O\lpar \log^{z+1} k\rpar$-approximation, the values $s(p)$ defined in \cref{alg:main} can be used to perform the coreset construction algorithm, and we conclude from \cref{fact:logApprox}.
\end{proof}

%
%\subsection{A Two-Step Algorithm}
%We present a different algorithm, based on the following standard transitivity property: if $\coreset_1$ is an $\eps$-coreset of $P$, and $\coreset_2$ an $\eps$-coreset of $\coreset_1$, then $\coreset_2$ is a $2\eps$-coreset of $P$.
%
%This property could be used as follows: computing a precise approximation for $P$ may be slow. Instead, one could spend the time budget as follows:
%
%\begin{enumerate}
%\item First, compute a rough approximation and use it to build an $\eps/2$-coreset $\coreset_1$. Due to the poor quality of approximation, this coreset may be larger than the target size; but still way smaller than $P$.
%\item  Then, compute a precise approximation on $\coreset_1$, and use it to build a smaller $\eps/2$-coreset $\coreset_2$ of $\coreset_1$. 
%\end{enumerate}
%By transitivity, $\coreset_2$ is therefore a small $\eps$-coreset for $P$. The bottleneck running time, to compute the precise approximation, now depends merely on $|\coreset_1|$ instead of $P$: one can therefore hope to be able to compute a much more precise estimations of the sensitivities required to sample a small coreset afterwards.
%
%The theoretical result does not improve compared to \cref{thm:main}, but this algorithm may be more practical. Therefore, we try it as well in our experiments.
%
%\david{add something depending on the results of experiments}

\subsection*{Extensions.} 
We conclude this section with a few remarks that allows to generalize \cref{alg:main}.

Second, one could compute the solution $\calC$ via a different algorithm: \cref{alg:main} only needs to be provided with an assignment to a solution that is a $O(\polylog k)$ approximation. 
This initial solution may as well be an $O\lpar\polylog \lnor P \rnor_0 \rpar$ approximation: using the iterative coreset construction from \cite{BravermanJKW21}, one could then derive a near-optimal coreset size, only suffering a $O(\log^* n)$ loss in the running time.

As an example, we illustrate a different approach for $k$-median. One could first embed the input into a hierarchically separated tree (HST) with expected distortion $O(\log \lnor P \rnor_0)$ \cite{FakcharoenpholRT03}: on such tree metrics, solving $k$-median can be done in linear time, using dedicated algorithms (see e.g. \cite{Cohen-AddadLNSS21}). Using the solution from the HST metric, one can compute a coreset, and iterate using the previous argument.
This embedding into HST is very similar to what is done by the \fkmeans algorithm, but can be actually performed in \emph{any} metric space, not only Euclidean. 
For instance, in a metric described by a graph with $m$ edges, the running time of this construction would be near linear-time $\tilde O(m)$. %In Euclidean space, the standard implementation runs in time $\Omega(n^2)$: we therefore omit it from our experiments.
