\section{Coresets in $\tilde{O}(nd)$ time}

In this section, we present our algorithm and assert that it produces a strong coreset with high-probability in time $\tilde{O}(nd)$.
The proceeding sections are as follows. In section

Those are based on the following observations about the group
sampling \cite{stoc21} and sensitivity sampling \cite{FeldmanL11} coreset construction algorithms.

Both start by computing a solution $\calC$. When $\calC$ is a $c$-approximation, the two algorithms compute a $c \eps$-coreset of size $\tilde O\lpar
k \eps^{-z-2}\rpar$ and $\tilde O\lpar k \eps^{-2z -2}\rpar$, respectively. Hence, by rescaling, they provide $\eps$-coreset with size $\tilde O\lpar
k (\eps/c)^{-z-2}\rpar$ and $\tilde O\lpar k (\eps/c)^{-2z-2}\rpar$. 
%\footnote{For group sampling, this result is not formally stated, but can be observed in Definition 1 and Lemma 12 of \cite{stoc21}: if $\calC$ is a $c$ approximation, rescaling $\eps$ by $c$ provides an approximate centroid set of the right precision, and the right error on the coreset. For sensitivity sampling, this can be seen in Lemma 15.4 of \cite{FLArxiv}, where $B$ is our $\calC$: if it is a $c$ approximation, rescaling $\eps$ by $c$ ensures an error $\eps \opt$ in the last line of the proof.} 

This leads to the following fact:
\begin{fact}\label{fact:logApprox}
Let $\calC$ be an $O\lpar \log^{O(1)} k\rpar$ approximation to $k$-median or $k$-means.
Then, group sampling using solution $\calC$ computes a coreset of size $\tilde O\lpar
k \eps^{-z-2}\rpar$, and sensitivity sampling one of size $\tilde O\lpar k \eps^{-2z-2}\rpar$. 
Both runs in time $\tilde O(nd)$, provided $\calC$.
\end{fact}
%Therefore, taking $c = O(\log k)$ merely affects the size of the computed coreset, while allowing for approximation algorithms that are way faster.

%\david{remove the next two paragraphs?}
%Due to the difficulty of verifying coresets, it is important to have a theoretical proof of efficiency of the coreset algorithms.  Showing, as we do, that one can use $O(\log k)$-approximations (e.g., $k$-means++) while benefiting from the same
%guarantees as with a slow $O(1)$-approximation is thus not only an artifact, but a crucial piece of coreset's success. 
%
%It is indeed tempting to use those large-factor approximations, especially since they seem to perform better in practice. 
%It might however have been the case that coresets constructed that way have poor quality, without being able to notice it due to the aforementioned hardness. 
%We show that this unfortunate event does not happen, and that, even with the ``worst-case" guarantee $O(\log k)$, these algorithms indeed produce coresets.

\subsection{HSTs and Fast-$k$Means++}
Our algorithm is based on the \fkmeans algorithm from \cite{cohen2020fast}: it allows to embed points in a very simple metric,  where it is easy to compute the
distance to any given set of centers. The two key properties of this algorithm are the following: 
\begin{itemize}
\item \fkmeans runs in $\tilde O\lpar n d \log \Delta\rpar$ time (Corollary 4.3 in \cite{cohen2020fast}, and
\item \fkmeans computes an assignment from input point to centers that is a $O\lpar d^z \log k\rpar$ approximation to $k$-median ($z=1$) and
$k$-means ($z=2$) (Lemma 3.1 in \cite{cohen2020fast} for $z=2$, discussion above for $z=1$). Applying dimension reduction techniques \cite{MakarychevMR19}, the dependency on $d$ may be replaced by a dependency on $\log k$. This results in a $O\lpar\log^{z+1} k\rpar$ approximation.
\end{itemize}

The second property is crucial for us: the algorithm does not only compute centers, but also assignments. 
  We use it, in combination with sensitivity sampling, as described in \cref{alg:main}.


\begin{algorithm}[tb]
   \caption{Fast-Coreset}
   \label{alg:main}
\begin{algorithmic}[1]
   \State {\bfseries Input:} data $P$, number of clusters $k$, precision $\eps$ and target size $S$
   \State Use Johnson-Lindenstrauss embedding to compute the embedding $\tilde P$ of $P$ into $\tilde d = O(\log k)$ dimensions
   \State Compute an approximate solution $\tilde \calC = \lbra \tilde c_1, ..., \tilde c_k\rbra $ for $\tilde P$, and an assignment $\tilde \sigma : \tilde P \rightarrow \tilde \calC$ using \fkmeans.	
   \State Let $\calC_i = \tilde \sigma^{-1}(c_i)$.
   \State Compute the $1$-median (or $1$-mean) $c_i$ of each $\calC_i$ in $\R^d$.%, and define $\sigma(p) := c_i$ for all $p \in \calC_i$.
   \State For each point $p \in \calC_i$, define
   $s(p) = \frac{\dist^z(p, c_i)}{\cost(\calC_i, c_i)}+ \frac{1}{|\calC_i|}$.
   \State Compute a set $\coreset$ of $S$ points randomly sampled from $P$ proportionate to $s$.
   \State For each $\calC_i$, define $|\hat \calC_i|$ the estimated weight of $\calC_i$ by $\coreset$, namely $|\hat \calC_i| := \sum_{p \in \calC_i \cap \coreset} \frac{\sum_{p' \in P}s(p')}{s(p)S}$.
   \State {\bfseries Output:} the coreset $\coreset$, with weights $w(p) = \frac{\sum_{p' \in P}s(p')}{s(p)S} \lpar (1+\eps)|\calC_i| - |\hat \calC_i|\rpar$
\end{algorithmic}
\end{algorithm}

Given this algorithm, we formalize the proof of \cref{thm:main}.

\begin{proof}[Proof of \cref{thm:main}]
We show that \cref{alg:main} has the desired guarantees.
First, performing the Johnson-Lindenstrauss embedding takes time $\tilde O(nd)$.

On the projected dataset, the algorithm \fkmeans runs in time $\tilde O\lpar n \log \Delta\rpar$, and the solution it has an approximation-ratio $O\lpar \tilde{d}^z \log k\rpar = O\lpar\log^{z+1} k\rpar$ for $\tilde P$. 
The guarantee offered by the embedding ensure that the clustering $\lbra \calC_1,...,\calC_k\rbra$ still has approximation ratio for $P$ \cite{makarychev2019performance}. 

For $k$-means, computing the $1$-mean solution for each $\calC_i$ takes time $O(nd)$ (the $1$-mean is simply the mean). 
For $k$-median, computing the $1$-median solution can be done as well in time $O(nd)$ \cite{CohenLMPS16}. 
We note that both may be approximated to a factor $2$ in constant time, by sampling uniformly at randm few points from each cluster \cite{neurips21}.

Provided the $c_i$ and the partition $\calC_i$, computing $|\calC_i|$ and $\cost(\calC_i, c_i)$ for all $i$ also takes time $O(nd)$.

Since the solution consisting of assigning each $p \in \calC_i$ to $c_i$ is a $O\lpar \log^{z+1} k\rpar$-approximation, the values $s(p)$ defined in \cref{alg:main} can be used to perform the coreset construction algorithm, and we conclude from \cref{fact:logApprox}.
\end{proof}

%
%\subsection{A Two-Step Algorithm}
%We present a different algorithm, based on the following standard transitivity property: if $\coreset_1$ is an $\eps$-coreset of $P$, and $\coreset_2$ an $\eps$-coreset of $\coreset_1$, then $\coreset_2$ is a $2\eps$-coreset of $P$.
%
%This property could be used as follows: computing a precise approximation for $P$ may be slow. Instead, one could spend the time budget as follows:
%
%\begin{enumerate}
%\item First, compute a rough approximation and use it to build an $\eps/2$-coreset $\coreset_1$. Due to the poor quality of approximation, this coreset may be larger than the target size; but still way smaller than $P$.
%\item  Then, compute a precise approximation on $\coreset_1$, and use it to build a smaller $\eps/2$-coreset $\coreset_2$ of $\coreset_1$. 
%\end{enumerate}
%By transitivity, $\coreset_2$ is therefore a small $\eps$-coreset for $P$. The bottleneck running time, to compute the precise approximation, now depends merely on $|\coreset_1|$ instead of $P$: one can therefore hope to be able to compute a much more precise estimations of the sensitivities required to sample a small coreset afterwards.
%
%The theoretical result does not improve compared to \cref{thm:main}, but this algorithm may be more practical. Therefore, we try it as well in our experiments.
%
%\david{add something depending on the results of experiments}


\section{Reducing the Impact of the Spread}
To reduce the dependency of $\Delta$ on the running time, we proceed in two steps: first, we compute a very crude upper-bound on the cost $U$ of the optimal solution -- up to a $\poly(n)$ factor. 
If $U$ is a $c$-approximation of the optimal cost, the natural attempt to reduce the aspect-ratio is to round all coordinates to multiples of $U/(cn√†$, such that the smallest distance is $U/(cn)$; it is then enough to reduce the diameter to $\poly(n) U$. We proceed as follows: we place a grid with cell length $O(n \cdot U)$, so that two points from the same cluster in $\opt$ fall into the same cell w.h.p. That way, the distinct cells do not interact with each other in any reasonable solution. 
Then, we compress the input by "moving" non-empty cells closer to each other.

\subsection{Computing a crude upper-bound}
As described, we start by computing a $U$ such that $U \leq \poly(n) \cdot \cost(\opt)$. For this, we start by simplifying a lot our task: our first lemma shows that it is enough focus on the $k$-median problem; we then embed the input into a tree, easier to handle than Euclidean space. In this tree, we will show that it is enough to find the first level for which the input lies in $k+1$ disjoint subtrees: the final algorithm is therefore a mere binary-search to find efficiently this level.

\paragraph{Reduction to $k$-median.}

\begin{lemma}
Let $\calS$ be a $c$-approximation for $k$-median on $P$. Then, $\calS$ is a $nc^2$-approximation for $k$-means on $P$.
\end{lemma}
\begin{proof}
Let $\cost_1$ (resp. $\cost_2$) be the $k$-median (resp. $k$-means) cost, $\opt_1$ (resp. $\opt_2$) be the optimal $k$-median (resp. $k$-means) solution. We have the following inequalities:
\begin{align*}
\cost_2(\calS) &= \sum_{p\in P} \dist(p, \calS)^2 \leq \lpar \sum_{p\in P} \dist(p, \calS)\rpar^2\\
&\leq c^2 \cdot \lpar  \sum_{p\in P} \dist(p, \opt_1)\rpar^2\\
&\leq c^2 \cdot \lpar  \sum_{p\in P} \dist(p, \opt_2)\rpar^2\\
&\leq c^2 \cdot n \cdot  \sum_{p\in P} \dist(p, \opt_2)^2,
\end{align*}
where the last inequality stems from Cauchy-Schwarz. Therefore, $\calS$ is a $nc^2$ - approximation to $k$-means. 
\end{proof}

\paragraph{Reduction to Structured Tree.}
\newcommand{\boxsize}{\textsc{MaxDist}}
It is well known that, for $k$-median, embedding the input into a tree using the quadtree decomposition loses only a $O(d \log \Delta)$ factor. Building explicitely this embedding takes time $O(nd \log \Delta)$: however, we will show that we only need to build only a fraction of it, allowing for a running time $O(nd \log \log \Delta)$. 

The embedding is constructed as follows. First, the algorithm finds a box enclosing all input points, centered at zero, with all side length equal to $\boxSize$.\footnote{This can be done as follows: select an arbitrary input point, and translate the dataset so that this point is at the origin. Then, compute the maximum distance from any point to the origin in time $O(nd)$, and let $\boxSize$ be that distance.} Then, a random shift $s \leq \boxsize$ is added to all coordinates of all points: the input is now in the box $[-2\boxsize, 2\boxsize]^d$ and this transformation did not change any distances, henceforth the $k$-median cost is preserved. 
The $i$-th level of the tree (for $i \in \lbra 0, ..., \log \Delta \rbra$) is constructed as follows: a grid of side length $2^{-i} \cdot 2\boxsize$ centered at $0$ is placed, and each cell is a node in the tree.
 The parent of a cell $c$ is simply the cell at level $i-1$ that contains $c$, and the distance between $c$ and its parent is set to $\sqrt{d}2^{-i} \cdot 2\boxsize$ (namely, the $\ell_2$ diameter of $c$).
 
 This tree has $O(\log \Delta)$ levels, and it is known that for any solution $\calS$, its cost in the tree metric is within a factor $O(\sqrt d \log \Delta)$ of its cost for the original metric. Therefore, it is enough for us to find an approximation in the tree metric. For this, we let $\opt_T$ be the optimal solution in the tree metric, and have the following lemma:
 
\begin{lemma}\label{lem:apxTree}
Let $i$ be the first level of the decomposition such that at least $k+1$ cells at level $i$ contains any point. Then, $\sqrt{d}2^{-i+1} \cdot \boxsize \leq \opt_T \leq n \cdot \sqrt{d}2^{-i+4} \cdot \boxsize$.
\end{lemma}
\begin{proof}
If the input is spread in $k+1$ distinct cell at level $i$, then in any solution there is at least one of those cells with no center. In the tree metric, points lying in this cell have therefore connexion cost at least $\sqrt{d}2^{-i+1} \cdot \boxsize$, and thus the left-hand-side of the inequality holds.

On the other hand, if the input is contained into $k$ cells at level $i-1$, then placing arbitrarily a center in each cell yields a solution with cost at most $n \cdot \sqrt{d}2^{-i+4} \cdot \boxsize$: indeed, the distance from any point to its closest center is at most $2 \cdot \sum_{j \geq i-1} \sqrt{d}2^{-j} \cdot 2\boxsize \leq 4 \sqrt{d}2^{-i+2} \cdot \boxsize$ (summing the edge length from the point to the cell at level $i$, and then going down to the center). This concludes.
\end{proof}

A consequence of \cref{lem:apxTree} is that the first level of the tree for which at least $k+1$ cells are non empty provides an $O(n)$-approximation. 
To count the number of non-empty cells at a given level $i$, one can merely iterate over all points, for each point identify the cell that contains it (using modulo operation), and store all those cells into a hash table to count the number of elements. This is done in time $O(nd)$.
Using a binary search on the $O(\log \Delta)$ many levels then concludes this section with the following result:

\begin{lemma}\label{lem:crudeApx}
There is an algorithm running in time $O(nd \log \log \Delta)$ that computes an  $O(n^2 d \log \Delta)$-approximation to $k$-median or $k$-means.
\end{lemma}

\subsection{From Approximate Solution to Small Aspect-Ratio}
Let $U$ be an upper-bound on the optimal cost that is a $c$-approximation, computed via \cref{lem:crudeApx}. We place a grid with side length $d n^2\cdot U$.
The following folklore lemma ensures that with high probability, no cluster of the optimal solution is spread on several grid cells.

\begin{lemma}
The probability that two points $p$ and $q$ are in different grid cells is $O(\frac{\|p-q\|^2}{n^2 U}$
\end{lemma}
Since $U$ is larger than the distance between any input point and its center in the optimal solution, a union-bound ensures that with probability $1-1/n$, no cluster of this solution is split among different cells.
In particular, there are at most $k$-non empty cells. We call those "boxes".

From this input, we build a new one $P'$ as follows: first, identify the non empty cell (using a hash table as previously). We associate each box with its center.
For each coordinate $i \in \lbra 1, ..., d \rbra$, sort (in time $O(k \log k)$ the centers according to their value on coordinate $i$. Then, for each $j \in \lbra 1, ..., k\rbra$, let $c^i_j$ and $c^i_{j+1}$ be the $i$-th coordinate of centers of the $j$-th and $(j+1)$-th boxes. If $c^i_{j+1} - c^i_j \geq 2d n^2\cdot U$, then for all cell $j'$ with $j' > j$, shift points  of $j'^$ by an amount $c^i_{j+1} - c^i_j - 2d n^2\cdot U$ in $i$-th coordinate.


This can be implemented with a linear scan, and has two effects: first, the diameter of the input is now $\sqrt{d} \cdot 2d n^2\cdot U \cdot k$, as along any coordinate the maximal distance is $2d n^2\cdot U \cdot k$. Second, two boxes that were adjacents are still adjacents, and two boxes that were non-adjacents are still non-adjacent.

The first property allows to reduce the spread to $(nd \log \Delta)^{O(1)}$. Indeed, one can round all coordinates to the closest multiple of $\frac{U}{n^4 d^{2} \log \Delta}$. That way, each point is at distance at most $\frac{U}{n^4 d^{2} \log \Delta}$ of its rounding. using \cref{lem:crudeApx}, $U \leq n^2 d \log(\Delta) \opt$, and therefore the distance between any point and its rounding is at most $\frac{\opt}{n^2}$. Summing this error over all points, this ensures that any solution computed on the rounding has cost within an additive factor $\pm \frac{\opt}{n}$ of the true cost. Furthermore, the smallest non-zero distance is $\frac{U}{n^4 d^{2} \log \Delta}$, and therefore the spread of the new metric is $2 n^6 d k \log \Delta = (nd \log \Delta)^{O(1)}$, as claimed.

The second property, on the other hand, ensures that we can transform a solution $\calS'$ for $P'$ to a solution with exactly the same cost for $P$: in any (reasonable) solution, points from two non-adjacent box will not be in the same cluster in $P'$ nor in $P$. Therefore, simply adding to centers of $\calS'$ the corresponding shift allows to transform it to a solution $\calS$, such that the distance between any point and its closest center does not change. This is formalized in the next lemma.

\begin{lemma}
Let $\calS'$ be a $c'$-approximation for  $k$-median (resp. $k$-means) on $P'$, where $c' \leq nc$ and $c$ is the approximation guarantee from \cref{lem:crudeApx}. Then, one can compute a solution for $P$ for $k$-median (resp. $k$-means) on $P$, with same cost as $\calS'$ for $P'$, in time $O(nd)$.
\end{lemma}
\begin{proof}
First, since distances in $P'$ are smaller than in $P$, the optimal solution for $P'$ has cost at most $U$. Therefore, two points that are in non-adjacent boxes (i.e., at distance more than $d n^2\cdot U$) are not in the same cluster of $\calS'$ -- as otherwise $\calS'$ would not be a $c'$-approximation.

Let $\calS$ be the solution obtained from $\calS'$ by reversing the construction of $P'$. Since this construction preserves adjacency, for all cluster of the solution, all distances are the same in $P$ and $P'$: therefore, the cost are alike. This concludes.
\end{proof}


\subsection*{Extensions.} 
We conclude this section with a few remarks that allows to generalize \cref{alg:main}.

Second, one could compute the solution $\calC$ via a different algorithm: \cref{alg:main} only needs to be provided with an assignment to a solution that is a $O(\polylog k)$ approximation. 
This initial solution may as well be an $O\lpar\polylog \lnor P \rnor_0 \rpar$ approximation: using the iterative coreset construction from \cite{BravermanJKW21}, one could then derive a near-optimal coreset size, only suffering a $O(\log^* n)$ loss in the running time.

As an example, we illustrate a different approach for $k$-median. One could first embed the input into a hierarchically separated tree (HST) with expected distortion $O(\log \lnor P \rnor_0)$ \cite{FakcharoenpholRT03}: on such tree metrics, solving $k$-median can be done in linear time, using dedicated algorithms (see e.g. \cite{Cohen-AddadLNSS21}). Using the solution from the HST metric, one can compute a coreset, and iterate using the previous argument.
This embedding into HST is very similar to what is done by the \fkmeans algorithm, but can be actually performed in \emph{any} metric space, not only Euclidean. 
For instance, in a metric described by a graph with $m$ edges, the running time of this construction would be near linear-time $\tilde O(m)$. %In Euclidean space, the standard implementation runs in time $\Omega(n^2)$: we therefore omit it from our experiments.
