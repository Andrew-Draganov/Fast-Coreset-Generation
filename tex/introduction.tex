\section{Introduction}

The modern data analyst has no shortage of clustering algorithms to choose from but, given the ever-increasing size of relevant datasets, many are often
rendered unviable due to time constraints. This has prompted the rise of big data algorithms which provide both theoretical guarantees and
practical improvements for standard data-science techniques on otherwise insurmountable datasets. The perspectives
of theoretical soundness and practical efficacy are, however, slightly at odds with one another. On the one hand, theoretical guarantees provide assurance that
the algorithm will obtain valid solutions without unlucky inputs affecting their runtime or accuracy. On the other hand, it is sometimes difficult to convince
oneself to implement the theoretically optimal algorithm when there are cruder methods that are faster to get running and perform well in practice.

Since datasets can be large in the number of points $n$ and/or the number of features $d$, big-data methods must mitigate the effect of large values of both.
With respect to the feature space, the question is effectively closed as random projections are fast -- running in effectively linear time, practical to
implement, and provide tight guarantees on the embedding's size and quality. The outlook is less clear when reducing the number of points $n$, with
two separate paradigms providing distinct advantages.  On the one hand we have uniform sampling, which runs in sublinear time but may miss important subsets of
the data and therefore cannot guarantee accuracy.  On the other hand the most accurate sampling strategies are those that provide the \emph{strong coreset}
guarantee, wherein the cost of any solution on the compressed data is within an $\varepsilon$-factor of that solution's cost on the original dataset.
Surprisingly, many coreset constructions can remove the dependency on $n$, significantly accelerating downstream tasks.

We study both paradigms with respect to a classic problem -- what are the limits and possibilities of compression for the $k$-means and $k$-median objectives?
Whereas uniform sampling provides optimal speed with no guarantees regarding size vs. accuracy, coreset constructions have
unclear runtime bounds despite their tight guarantees on the minimum number of samples required. Although there is little to study with respect to random
sampling, the lack of clarity regarding linear-time coreset constructions throws into question their ability to facilitate clustering on large datasets.
Indeed, it has been conjectured that producing a $k$-means or $k$-median coreset necessitates an $\tilde{O}(nd + nk)$ runtime.
Since the issue of optimal coreset sizes has been resolved, this is one of the main open problems regarding center-based coresets. To this end, we show that
there does exist an easy-to-implement algorithm that constructs coresets in $\tilde{O}(nd)$ time -- only logarithmic factors away from the time it takes to read
in the dataset.

Nonetheless, this does not fully illuminate the landscape between the sampling paradigms for clustering algorithms. Although our algorithm achieves an optimal
runtime, it is certainly possible that other, cruder methods are equally viable in practice despite being faster and easier to implement. Other than uniform
sampling, this includes methods such as Lightweight Coresets, which do not satisfy the strong coreset guarantee and simply use the distance to the mean
of the dataset. Thus, there is a natural spectrum within the linear-time algorithms where obtaining more information about the dataset allows for a better
sampling strategy at the expense of sublinear factors in speed. We state this formally in the following question: When are optimal $k$-means and $k$-median
coresets necessary and what is the practical tradeoff between coreset speed and accuracy? To this end, we show that while many practical settings do not require
the full coreset guarantee, one cannot cut any corners regarding the strong coreset constructions if one wants to be confident in their compression. We also
show that this extends to the streaming data paradigm and applies across downstream $k$-means and $k$-median approaches.
