\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{makecell}
\usepackage{caption}
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath} \usepackage{amssymb} \usepackage{mathtools} \usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{question}[theorem]{Question}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}




\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator{\cost}{cost}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\poly}{poly}


\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\coreset}{\Omega}
\newcommand{\calC}{\mathcal C}
\newcommand{\calS}{\mathcal S}
\newcommand{\calA}{\mathcal A}
\newcommand{\opt}{\text{OPT}}
\newcommand{\eps}{\varepsilon}

\newcommand{\fkmeans}{\textsc{Fast-kmeans++}~}

\newcommand{\lpar}{\left(}
\newcommand{\rpar}{\right)}
\newcommand{\lbra}{\left\{}
\newcommand{\rbra}{\right\}}
\newcommand{\lnor}{\left\|}
\newcommand{\rnor}{\right\|}


\newcounter{sideremark}
\newcommand{\marrow}{\stepcounter{sideremark}\marginpar{$\boldsymbol{
\longleftarrow\scriptstyle\arabic{sideremark}}$}}

 \newcommand{\david}[1]{
 %  \ifdraft{
    \textsf{{\color{blue} *** (David) \marrow #1 ***}}
 %  }
 %  \fi
 }
  \newcommand{\andrew}[1]{
 %  \ifdraft{
    \textsf{{\color{red} *** (Andrew) \marrow #1 ***}}
 %  }
 %  \fi
 }
 
   \newcommand{\chris}[1]{
 %  \ifdraft{
    \textsf{{\color{green} *** (Chris) \marrow #1 ***}}
 %  }
 %  \fi
 }

 
%for having enumerates with (i) (ii) (iii): 
 \renewcommand{\labelenumi}{\theenumi}
 
% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
%\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{algorithm}
\usepackage{algpseudocode}


\title{Settling Time vs. Accuracy Tradeoffs for Clustering Big Data}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}

We study the theoretical and practical limits of $k$-means and $k$-median clustering on large datasets. Since the reader's favorite clustering method is likely
slower than $\tilde{O}(nd)$ time, the general approach is to compress the data and perform the clustering on the compressed representation. Towards this goal,
the number of features can be reduced in effectively linear time with guaranteed accuracy using random projections. Unfortunately, there is no such universal
best choice for compressing the number of points -- while random sampling runs in sublinear time and coresets provide theoretical guarantees, the former does
not enforce accuracy while the latter is too slow as $n$ and $k$ grow. Indeed, it has been conjectured that any sensitivity-based coreset construction
necessitates $\tilde{O}(nd + nk)$ time.

We examine this relationship by first showing that there does exist an algorithm that obtains coresets via sensitivity sampling in $\tilde{O}(nd)$ time --
within log-factors of the time it takes to read the data.  Any approach that significantly improves on this must then resort to practical heuristics, leading us
to consider the spectrum of sampling strategies across both real and artificial datasets in the static and streaming settings. Through this, we show the
conditions in which coresets are necessary for preserving cluster validity as well as the settings in which faster, cruder sampling strategies are sufficient.
As a result, we provide a comprehensive theoretical and practical blueprint for clustering effectively regardless of data size.  Our (anonymized) code is
publicly available at \_\_\_\_.

\end{abstract}


\input{introduction}
\input{preliminaries}
%\input{related_work}
\input{theory}
%\input{spread}
\input{results}



% Acknowledgements should only appear in the accepted version.

\bibliography{references}

\appendix
\input{appendix}
\end{document}
