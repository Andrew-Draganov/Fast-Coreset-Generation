\section{Proofs, Pseudo-Code, and Extensions}

We put the proofs, pseudo-code and algorithmic extensions towards the end of the paper for improved readability of the primary text.  Algorithm
\ref{alg:crudeApx} corresponds to the discussion in Section~\ref{ssec:crude_bound} and Algorithm~\ref{alg:reduce-diam} corresponds to the discussion in
Section~\ref{ssec:reduce_spread}.

\label{app:pseudoCode}
\begin{algorithm}[tb]
   \caption{Crude-Approx($P$)}
   \label{alg:crudeApx}
\begin{algorithmic}[1]
\Procedure{Count-Distinct-Cells}{$P, c, \ell$} \Comment data $P$, $c$ is the center of the quadtree grid at level $\ell$
   \State Let $\calD$ be a dictionnary, and count $= 0$.
   \For{each point $p \in P$}
   \State let $c^p$ be the center of the cell containing $p$. The $i$-th coordinate of $c_p$ is $\lfloor \frac{p_i - c_i}{2^\ell}\rfloor  \cdot 2^\ell + \frac{2^{\ell}}{2}$. 
   \If{$c_p$ is a not a key of $\calD$} 
   \State insert $c_p$ in $\calD$ and do count $\gets$ count $+1$.
   \EndIf
   \EndFor
   \State {\bfseries Output:} True if count $\geq k+1$, False otherwise.
   \EndProcedure
   
   \Procedure{Crude-Approx}{$P$} \Comment data $P$, with diameter $\Delta$.
   \State let $s$ be a u.a.r in $[0, \Delta]^d$, and $c = (s,..., s) \in \R^d$.
   \State using a binary search, find the smallest $\ell$ such that Count-Distinct-Cells($P, c, \ell$) = True. Let $\ell_0$ be that level.
   \State \textbf{Output:} $U = 2^{\ell_0} \Delta$.
   \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
   \caption{Reduce-Spread($P, U$)}
   \label{alg:reduce-diam}
\begin{algorithmic}[1]
\Procedure{Reduce-Diameter}{$P, U$} \Comment data $P$, upper bound on $\opt$ $U$
   \State Let $\calD$ be a dictionnary
   \State Let $r = \sqrt d n^2\cdot U$, $s$ be chosen u.a.r. in $\{0, ..., r\}$, and $c = {s, ..., s}$ be the center of the grid.
   \For{each point $p \in P$}
   \State let $c^p$ be the center of the cell containing $p$. The $i$-th coordinate of $c_p$ is $\lfloor \frac{p_i - c_i}{r}\rfloor  \cdot r + \frac{r}{2}$.
   Add $p$ to $\calD[c_p]$.
   \EndFor
   \State Identify the non-empty dictionnary keys $c^1, ..., c^k$, and let $\calC_1, ..., \calC_k$, be the corresponding cells.
   \For {each coordinate $i = 1, ..., d$}
   \State Sort the cells according to the $i$-th coordinate of their center. Let $\delta = 0$. 
   \For{$j = 1, ..., k$}
    \If {$\frac{c^j_i - c^j_{i-1}}{r} \geq 2$} update $\delta \gets \delta + c^j_i - c^j_{i-1} - 2r$.
	\EndIf    
    \State Substract $\delta$ from the $i$-th coordinate of all points in the $j$-th cell. 
    \EndFor
   \EndFor
   \State {\bfseries Output:} the dataset $P'$ consisting of all shifted points.
   \EndProcedure
   
   \Procedure{Reduce-Min-Distance}{$P, U$} \Comment data $P$, $U$ such that $U \leq n^2 d \log(\Delta) \opt$.
   \State Round each coordinates of points to the closest multiple of $g := \frac{U}{n^4 d^2 \log \Delta}$.
   \State \textbf{Output:} the dataset $P'$ with all point after rounding.
   \EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Proofs from Sections \ref{sec:theory_0}, \ref{sec:theory}}\label{app:theory}
\subsection{Proof of Corollary~\ref{cor:mainAlg}}
\label{ssec:cor_proof}
Recall that Corollary~\ref{cor:mainAlg} states that \cref{alg:main} produces an $\eps$-coreset in time $\tilde O(nd \log \Delta)$.
\begin{proof}[Proof of \cref{cor:mainAlg}]
%We show that \cref{alg:main} has the desired guarantees.
First, performing the Johnson-Lindenstrauss embedding takes time $\tilde O(nd)$.

On the projected dataset, the algorithm \fkmeans runs in time $\tilde O\lpar n \log \Delta\rpar$, and the solution it has an approximation-ratio $O\lpar \tilde{d}^z \log k\rpar = O\lpar\log^{z+1} k\rpar$ for $\tilde P$. 
The guarantee offered by the embedding ensure that the clustering $\lbra \calC_1,...,\calC_k\rbra$ still has approximation ratio for $P$ \cite{makarychev2019performance}. 

For $k$-means, computing the $1$-mean solution for each $\calC_i$ takes time $O(nd)$ (the $1$-mean is simply the mean). 
For $k$-median, computing the $1$-median solution can be done as well in time $O(nd)$ \cite{CohenLMPS16}. 
We note that both may be approximated to a factor $2$ in constant time, by sampling uniformly at randm few points from each cluster \cite{neurips21}.

Provided the $c_i$ and the partition $\calC_i$, computing $|\calC_i|$ and $\cost(\calC_i, c_i)$ for all $i$ also takes time $O(nd)$.

Since the solution consisting of assigning each $p \in \calC_i$ to $c_i$ is a $O\lpar \log^{z+1} k\rpar$-approximation, the values $s(p)$ defined in \cref{alg:main} can be used to perform the coreset construction algorithm, and we conclude from \cref{fact:logApprox}.
\end{proof}


\subsection{Reduction of $k$-means to $k$-median.}
\label{app:redKM}
In our argument, the only step spectific to $k$-median is to compute the upper-bound $U$ on the cost of the solution. Provided such an upper-bound, rounding points and shifting the box would work exactly alike for $k$-means. Therefore, the next lemma is enough to extend our reduction of the spread to $k$-means:
\begin{lemma}\label{lem:kmedTokmeans}
Let $\calS$ be a $c$-approximation for $k$-median on $P$. Then, $\calS$ is a $nc^2$-approximation for $k$-means on $P$.
\end{lemma}
\begin{proof}
Let $\cost_1$ (resp. $\cost_2$) be the $k$-median (resp. $k$-means) cost, $\opt_1$ (resp. $\opt_2$) be the optimal $k$-median (resp. $k$-means) solution. We have the following inequalities:
\begin{align*}
\cost_2(\calS) &= \sum_{p\in P} \dist(p, \calS)^2 \leq \lpar \sum_{p\in P} \dist(p, \calS)\rpar^2\\
&\leq c^2 \cdot \lpar  \sum_{p\in P} \dist(p, \opt_1)\rpar^2\\
&\leq c^2 \cdot \lpar  \sum_{p\in P} \dist(p, \opt_2)\rpar^2\\
&\leq c^2 \cdot n \cdot  \sum_{p\in P} \dist(p, \opt_2)^2,
\end{align*}
where the last inequality stems from Cauchy-Schwarz. Therefore, $\calS$ is a $nc^2$ - approximation to $k$-means. 
\end{proof}

\subsection{Estimation of the Optimal Cost in a Tree}
\label{app:apx-tree-proof}

We conclude by proving Lemma~\ref{lem:apxTree}:
\begin{proof}[proof of \cref{lem:apxTree}]
If the input is spread in $k+1$ distinct cell at level $i$, then in any solution there is at least one of those cells with no center. In the tree metric, points
lying in this cell have therefore connexion cost at least $\sqrt{d}2^{-i+1} \cdot \Delta$, and thus the left-hand-side of the inequality holds.

On the other hand, if the input is contained into $k$ cells at level $i-1$, then placing arbitrarily a center in each cell yields a solution with cost at most
$n \cdot \sqrt{d}2^{-i+4} \cdot \Delta$: indeed, the distance from any point to its closest center is at most $2 \cdot \sum_{j \geq i-1} \sqrt{d}2^{-j} \cdot
2\Delta \leq 4 \sqrt{d}2^{-i+2} \cdot \Delta$ (summing the edge length from the point to the cell at level $i$, and then going down to the center). This
concludes.
\end{proof}


\begin{proof}[Proof of \cref{lem:crudeApx}]
The running time of \textsc{Count-Distinct-Cells} in \cref{alg:crudeApx} is $\tilde O(nd)$, using a standard dictionnary datastructure. To compute $U$,
\textsc{Crude-Approx} makes $O(\log \log \Delta)$ calls to \textsc{Count-Distinct-Cells}, as there are $O(\log \Delta)$ levels. This concludes the complexity of
the algorithm.

The approximation guarantee for $k$-median directly stems from \cref{lem:apxTree} and \cref{lem:quadtreeDist}. For $k$-means, it is a consequence of \cref{lem:kmedTokmeans}.
\end{proof}


\subsection{Extensions to Algorithm \ref{alg:main}}
\label{app:extensions}

Consider that Algorithm \ref{alg:main} only needs to be provided with an assignment to a solution that is a $O(\polylog k)$ approximation, implying that one
could compute the solution $\calC$ via any algorithm that satisfies this.  Indeed, this initial solution may as well be an $O\lpar\polylog \lnor P \rnor_0 \rpar$
approximation, where $\lnor P \rnor_0$ is the number of non-zero samples in $P$. Using the iterative coreset construction from \cite{BravermanJKW21}, one could
then derive a near-optimal coreset size, only suffering a $O(\log^* n)$ loss in the running time.

As an example, we illustrate a different approach for $k$-median. One could first embed the input into a hierarchically separated tree (HST) with expected
distortion $O(\log \lnor P \rnor_0)$ \cite{FakcharoenpholRT03}. On such tree metrics, solving $k$-median can be done in linear time using dedicated algorithms
(see e.g. \cite{Cohen-AddadLNSS21}). Using the solution from the HST metric, one can compute a coreset, and iterate using the previous argument.  This embedding
into HST is very similar to what is done by the \fkmeans algorithm, but can be actually performed in \emph{any} metric space, not only Euclidean.  For instance,
in a metric described by a graph with $m$ edges, the running time of this construction would be near linear-time $\tilde O(m)$.

