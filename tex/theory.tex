\section{Fast-Coresets}
\label{sec:theory}

\begin{algorithm}[tb]
   \caption{Fast-Coreset($P, k, \eps, m$)}
   \label{alg:main}
\begin{algorithmic}[1]
   \State {\bfseries Input:} data $P$, number of clusters $k$, precision $\eps$ and target size $m$
   \State Use a Johnson-Lindenstrauss embedding to embed $\tilde P$ of $P$ into $\tilde d = O(\log k)$ dimensions
   \State Find approx. solution $\tilde \calC = \lbra \tilde c_1, ..., \tilde c_k\rbra $ on $\tilde P$ and assignment $\tilde \sigma : \tilde P \rightarrow
   \tilde \calC$ by \fkmeans.	
   \State Let $\calC_i = \tilde \sigma^{-1}(c_i)$. Compute the $1$-median (or $1$-mean) $c_i$ of each $\calC_i$ in $\R^d$.%, and define $\sigma(p) := c_i$ for all $p \in \calC_i$.
   \State For each point $p \in \calC_i$, define
   $s(p) = \frac{\dist^z(p, c_i)}{\cost(\calC_i, c_i)}+ \frac{1}{|\calC_i|}$.
   \State Compute a set $\coreset$ of $m$ points randomly sampled from $P$ proportionate to $s$.
   \State For each $\calC_i$, define $|\hat \calC_i|$ the estimated weight of $\calC_i$ by $\coreset$, namely $|\hat \calC_i| := \sum_{p \in \calC_i \cap
   \coreset} \frac{\sum_{p' \in P}s(p')}{s(p)m}$.
   \State {\bfseries Output:} the coreset $\coreset$, with weights $w(p) = \frac{\sum_{p' \in P}s(p')}{s(p)m} \lpar (1+\eps)|\calC_i| - |\hat \calC_i|\rpar$
\end{algorithmic}
\end{algorithm}


In this section, we first combine two existing results to produce a strong coreset in time $\tilde{O}(nd \log \Delta)$, where $\Delta$ is the aspect-ratio of
the input.  We show afterwards how to reduce the dependency in $\Delta$ to $\log \log \Delta$, giving the desired nearly-linear runtime.

Our method is based on the following observations about the group sampling \cite{stoc21} and sensitivity sampling \cite{FeldmanL11} coreset construction
algorithms. Both start by computing a solution $\calC$. When $\calC$ is a $c$-approximation, they compute a $c \eps$-coreset of size $\tilde O\lpar
k \eps^{-z-2}\rpar$ and $\tilde O\lpar k \eps^{-2z -2}\rpar$, respectively. Hence, by rescaling, they provide an $\eps$-coreset with size $\tilde O\lpar
k (\eps/c)^{-z-2}\rpar$ and $\tilde O\lpar k (\eps/c)^{-2z-2}\rpar$.  This leads to the following fact:

\begin{fact}\label{fact:logApprox}
Let $\calC$ be an $O\lpar \log^{O(1)} k\rpar$ approximation to $k$-median or $k$-means.
Then, group sampling using solution $\calC$ computes a coreset of size $\tilde O\lpar
k \eps^{-z-2}\rpar$, and sensitivity sampling one of size $\tilde O\lpar k \eps^{-2z-2}\rpar$. 
Both run in time $\tilde O(nd)$, provided $\calC$.
\end{fact}

To turn \cref{fact:logApprox} into an algorithm, we use the quadtree-based \fkmeans approximation algorithm from \cite{cohen2020fast}, which has two key
properties. \underline{First}, \fkmeans runs in $\tilde O\lpar n d \log \Delta\rpar$ time (Corollary 4.3 in \cite{cohen2020fast}). \underline{Second}, \fkmeans
computes an assignment from input points to centers that is an $O\lpar d^z \log k\rpar$ approximation to $k$-median ($z=1$) and $k$-means ($z=2$) (Lemma 3.1 in
\cite{cohen2020fast} for $z=2$, the discussion above for $z=1$). Applying dimension reduction techniques \cite{MakarychevMR19}, the dimension $d$ may be
replaced by a $\log k$. This results in a $O\lpar\log^{z+1} k\rpar$ approximation.

The second property is crucial for us: the algorithm does not only compute centers, but also assignments in $\tilde{O}(nd\log \Delta)$ time.  We describe how to
combine it with sensitivity sampling in \cref{alg:main} and prove in \cref{app:theory} that this computes an $\eps$-coreset in time $\tilde O(nd \log \Delta)$.
Furthermore, we generalize \cref{alg:main} to other fast $k$-median approaches in \cref{app:extensions}.

Thus, it is easy to combine existing results to obtain an $\eps$-coreset without an $\tilde{O}(nk)$ time-dependency.  However, our method thus far has only
replaced the $\tilde{O}(nd + nk)$ runtime by $\tilde{O}(nd \log \Delta)$. Indeed, the spirit of the issue remains -- this is not log-linear in the input size.
We verify this by devising a dataset that has $n - n'$ points uniformly in the $[-1, 1]^2$ square. Then, for $r \in \mathbb{Z}^+$, we produce a sequence of
points at $(0, 1), (0, 0.5), \cdots, (0, 0.5^r)$ and copy this sequence $n' / r$ times, each time with a different $x$ coordinate. The result is a dataset of
size $n$ where a quadtree embedding always has constant breadth but the branch depths (and correspondingly $\log \Delta$) grow linearly with $r$. The resulting
linear time-dependency can then be seen in Table \ref{tbl:logdelta}.

\section{Reducing the Impact of the Spread}
\label{sec:logdelta}
\newcommand{\boxsize}{\textsc{MaxDist}}

\paragraph*{Overview of the Approach}

To remove the $\log\Delta$ dependency, we proceed in two steps: first, we compute a very crude upper-bound on the cost $U$ of the optimal solution -- up to
a $\poly(n)$ factor.  If $U$ is a $c$-approximation of the optimal cost then the aspect-ratio can be reduced by rounding all coordinates to multiples of $g
= U/(cn)$, giving us a minimum distance of no less than $g$; it is then enough to reduce the diameter to $\poly(n) U$.  Thus, we place a grid with cell length
$O(n \cdot U)$, so that two points from the same cluster in $\opt$ fall into the same cell w.h.p., implying that distinct cells do not interact with each other
in any reasonable solution.  Then, we compress the input by "moving" non-empty cells closer to each other. We will focus this section on the simpler $k$-median
problem but show how to reduce $k$-means to this case in \cref{app:redKM}.

\paragraph*{Computing a crude upper-bound}

As described, we start by computing an approximate solution $U$ such that $U \leq \poly(n) \cdot \cost(\opt)$. For this, the first step is to embed the input
into a quadtree: this gives a lot of structure to the input, and preserves the cost up to a factor $O(d n \log \Delta)$ for $k$-median.  Our first lemma shows
that the necessary approximation exists at the first level for which the input lies in $k+1$ disjoint subtrees. 

\begin{lemma}\label{lem:apxTree}
Let $i$ be the first level of the decomposition such that at least $k+1$ cells at level $i$ contains any point. Then, $\sqrt{d}2^{-i+1} \cdot \boxsize \leq
\opt_T \leq n \cdot \sqrt{d}2^{-i+4} \cdot \boxsize$.
\end{lemma}

We prove this in Section \ref{app:apx-tree-proof} of the appendix. A direct consequence of this lemma is that the first level of the tree for which at least
$k+1$ cells are non empty provides an $O(n)$-approximation. To count the number of non-empty cells at a given level $i$, one can merely iterate over all points,
and, for each point, identify the cell that contains it (using modulo operations). One then stores all those cells into a hash table to count the number of
elements, all in $O(nd)$ time.  Using a binary search on the $O(\log \Delta)$ many levels then concludes this section with the following result:

\begin{lemma}\label{lem:crudeApx}
There is an algorithm running in time $O(nd \log \log \Delta)$ that computes an  $O(n^2 d \log \Delta)$-approximation to $k$-median or $k$-means.
\end{lemma}

\paragraph*{From Approximate Solution to Small Aspect-Ratio}
Let $U$ be an upper-bound on the optimal cost that is a $c$-approximation, computed via \cref{lem:crudeApx}. We place a grid with side length $d n^2\cdot U$.
The following folklore lemma ensures that with high probability, no cluster of the optimal solution is spread on several grid cells.

\begin{lemma}
The probability that two points $p$ and $q$ are in different grid cells is $O\lpar \frac{\|p-q\|^2}{n^2 U}\rpar$
\end{lemma}

Since $U$ is larger than the distance between any input point and its center in the optimal solution, a union-bound ensures that with probability $1-1/n$, no
cluster of this solution is split among different cells.  In particular, there are at most $k$-non empty cells which we call ``boxes''.
From this input, we build a new set of points $P'$ as follows: first, identify the non empty cell (using a hash table as previously). We associate each box with
its center.  For each coordinate $i \in \lbra 1, ..., d \rbra$, sort (in time $O(k \log k)$) the centers according to their value on coordinate $i$. Then, for
each $j \in \lbra 1, ..., k\rbra$, let $c^i_j$ and $c^i_{j+1}$ be the $i$-th coordinate of centers of the $j$-th and $(j+1)$-th boxes. If $c^i_{j+1} - c^i_j
\geq 2d n^2\cdot U$, then for all cells $j'$ with $j' > j$, shift the points of $j'$ by $c^i_{j+1} - c^i_j - 2d n^2\cdot U$ in the $i$-th coordinate.

This can be implemented with a linear scan and has two effects: first, the diameter of the input is now $\sqrt{d} \cdot 2d n^2\cdot U \cdot k$, as along any
coordinate the maximal distance is $2d n^2\cdot U \cdot k$. Second, two boxes that were adjacent are still adjacent and two boxes that were non-adjacent are
still non-adjacent.

The first property allows us to reduce the aspect-ratio to $(nd \log \Delta)^{O(1)}$.  Indeed, one can round all coordinates to the closest multiple of
$g = \frac{U}{n^4 d^{2} \log \Delta}$. Since every point has moved by at most $g$ and, using \cref{lem:crudeApx}, $U
\leq n^2 d \log(\Delta) \opt$, it is clear that the distance between any point and its rounding is at most $\frac{\opt}{n^2}$. Summing over all points,
any solution computed on the gridded data has cost within an additive factor $\pm \frac{\opt}{n}$ of the true cost. Furthermore, the smallest
non-zero distance is $g = \frac{U}{n^4 d^{2} \log \Delta}$, implying that the aspect-ratio of the new metric is $(nd \log \Delta)^{O(1)}$,
as claimed.

The second property, on the other hand, ensures that we can transform a solution $\calS'$ for $P'$ to a solution with exactly the same cost for $P$: in any
(reasonable) solution, points from two non-adjacent boxes will not be in the same cluster in either $P'$ or $P$. Therefore, adding back the shift to centers of
$\calS'$ allows us to transform it to a solution $\calS$. We note that the distance between any point and its closest center does not change. This is formalized
in the next lemma and proven in \cref{app:kmedian}.

\begin{lemma}
Let $\calS'$ be a $c'$-approximation for  $k$-median (resp. $k$-means) on $P'$, where $c' \leq nc$ and $c$ is the approximation guarantee from
\cref{lem:crudeApx}. Then, one can compute a solution for $P$ for $k$-median (resp. $k$-means) on $P$, with same cost as $\calS'$ for $P'$, in time $O(nd)$.
\end{lemma}
