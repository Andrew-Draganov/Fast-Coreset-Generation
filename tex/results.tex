\section{Fast Coresets in Practice}
Things to show:
\begin{enumerate}
    \item Introduce datasets and metric(s)
    \begin{enumerate}
        \item Artificial dataset that breaks uniform sampling
        \item Benchmark dataset from ESA paper
        \item Geometric dataset that breaks lightweight coresets
    \end{enumerate}
    \item Running sensitivity sampling with kmeans++ does not scale with $k$ but fast-kmeans++ solution does
    \item Uniform sampling and lightweight coresets both don't work on simple toy datasets
    \item The size of the coreset seems to make a difference when performing sensitivity sampling but not as much
          when doing lightweight coreset and uniform
\end{enumerate}

\begin{itemize}
\item \david{Explain more precisely the construction of artifical datasets: what are the gaussian parameters, what value of $c$}
    \andrew{done}
\item \david{Rework on the paragraph Additional evaluation on imbalanced clusters}
    \andrew{doing later tonight}
\item \david{Missing plots: effect of $j$ for cut-$k$-means}
    \andrew{Have them and just need to put them in}
\item \david{Change the plots to have better visual precision: put blob, geometric and $c$-outlier in a different plot. Maybe also write bico on a distinct table since it is so bad.}
    \andrew{Will do tomorrow morning}
\end{itemize}

We now provide an experimental analysis to assert our two principal claims. First, Fast-Coresets obtain solutions of equal quality to sensitivity sampling
without the linear time-dependency on the number of centers $k$. Second, of the coreset methods that are sublinear in $k$, only Fast-Coresets provide guarantees
that the resulting compression satisfies the coreset property. We proceed by introducing our experimental setup, including the metrics, datasets, and algorithms
being analyzed, followed by a discussion of results for both of the above claims.

\subsection{Experimental Setup}
\subsubsection{Metrics}

We analyze the coreset construction methods along two metrics -- coreset quality and construction time.  Although measuring runtime is standard, predicting coreset
quality is a more difficult task. Specifically, it is unclear how to confirm that a subset of points satisfies the coreset property over all solutions. To this
end, the authors in \cite{chrisESA} suggested reporting the following metric 
\[ \max \left( \dfrac{\cost(P, \calC_{\Omega})}{\cost(\Omega, \calC_{\Omega})}, \dfrac{\cost(\Omega, \calC_{\Omega})}{\cost(P, \calC_{\Omega})} \right),\]
where $\calC_{\Omega}$ is a candidate solution that was computed over the coreset.

We refer to this metric as the \emph{coreset distortion}. Naturally, values that are consistently close to $1$ suggest that solutions on the coreset are likely
viable on the full dataset.


\subsubsection{Algorithms}
\label{ssec:algorithms}

\chris{Not to say we should plagiarize, but can we write this similar to the ESA paper? In general I found the ESA paper had a lot more detail. Also, a number of the comments are wrong or misleading. BICO does not use BIRCH.}

We compare our method (Algorithm~\ref{alg:main}) against 5 different benchmark coreset constructions:
\begin{itemize}
        \item \emph{Standard uniform sampling}: choose a subset of $m \ll n$ points uniformly from $P$.
        \item \emph{Lightweight coreset}: find the mean $\mu$ of the dataset and obtain per-point sensitivity estimates by $\hat{s}(p) = 1/|P| + \cost(p, \mu) / \cost(P, \mu)$.
            The first term encourages uniform sampling while the second encourages sampling proportionate to the distance from the mean.
        \item \emph{Birch using Coresets (BICO)}: use BIRCH algorithm~\cite{birch} to create streaming coreset quickly. \david{could you say a bit more about this? At least, what's the running time and the theoretical guarantee?}
            \andrew{Will do later tonight.}
        \item \emph{Standard sensitivity sampling}: obtain an approximate $k$-means solution with $k$-means++ and sample a coreset proportionate to sensitivity as defined in \cref{eq:sensitivity}.
        \item \emph{Cut-$k$-means coreset}: for a given $j \in \{1,..., k\}$, find an approximate $j$-means solution with $j$-means++ algorithm, and sample
            a coreset proportionate to sensitivity as defined in \cref{eq:sensitivity}.\end{itemize}
All but BICO have been implemented by us in a single codebase. We use the BICO code from \cite{bico}.

We take a moment here to motivate the Cut-$k$-means coreset algorithm.  Consider that lightweight coresets are simply solving $1$-means to obtain sensitivity
values whereas sensitivity sampling is solving the $k$-means problem. Since the lightweight coreset has the advantage of being fast while sensitivity sampling
is more precise, one could hope to interpolate in order to get the best of the two worlds: computing an approximate $j$-means solution may allow to obtain
a more precise sampling distribution than that of lightweight-coresets, while being faster than sensitivity sampling. We analyze the different choices of $j$ in
Table~\ref{tbl:class-imbalance} and default to $j = \log k$ in the other experiments.

\subsubsection{Datasets}
\input{figures/coreset_size_on_quality.tex}

We employ several real and artificial datasets to evaluate the quality of a coreset. 

For our real-world data, we utilize the Adult~\cite{Dua:2019}, MNIST~\cite{mnist}, Song~\cite{song}, Census~\cite{census}, and Cover Type~\cite{covtype}
datasets, whose characteristics are summarized in Table~\cref{tbl:datasets} below.

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrr}
        Dataset & Points & Dim \\
        \hline
        \emph{Adult} & 48\,842 & 14 \\
        \emph{MNIST} & 60\,000 & 784 \\
        \emph{Song} & 515\,345 & 90 \\
        \emph{Cover Type} & 581\,012 & 54 \\
        \emph{Census} & 2\,458\,285 & 68
    \end{tabular}
    \caption{Description of real world datasets}
        \label{tbl:datasets}
\end{table}

To complement those, we use several artificial datasets:
\begin{itemize}
    \item \emph{c-outlier}: $n-c$ points in a single location and $c$ points placed at a large distance away, for some constant $c$. We default to $n=50\,000$,
        $d=50$, and $c=5$.
    \item \emph{Gaussian mixture}: multivariate Gaussians with varying cardinalities arranged randomly in high-dimensional space. 
        The datasets consist of $n=50\,000$ points in $d=50$ dimensions. Each cluster has a center sampled from a multivariate normal with mean $0$ and
        variance $1\,000$. The clusters are all then standard multivariate normal distributions around this mean. \david{how many points per clusters?}
    \item \emph{Geometric}: $n/2$ points at $(1, 0, 0, \cdots)$, $n/4$ points at $(0, 1, 0, \cdots)$, and so on.
    \item \emph{Benchmark}: A specific distribution of points introduced in \cite{chrisESA} as a good testbed for coreset algorithms. 
    It has the property that all reasonable solutions are of equal quality but are maximally far apart in the solution space. When creating the benchmark, we
    follow the advice in \cite{chrisESA} and produce three benchmarks of varying size before applying random offsets to each so that they may be separated.
\end{itemize}

In all real and artificial datasets, we add random uniform noise $\eta$ with $0 \leq \eta_i \leq 0.001$ in each dimension in order to make all points unique.
Unless specifically varying these parameters, we default all algorithms in~\ref{ssec:algorithms} to $k=100$ for the Adult, MNIST, and artificial datasets and
$k=500$ for the Song, Cover Type, and Census datasets. Our default coreset size is then $m = 40k$. We refer to the coreset size scalar as the \emph{$m$-scalar}.
We only run the dimension-reduction step on the MNIST dataset, as the remaining ones already have sufficiently low dimensionality.

\subsection{Algorithm Comparisons}
\label{ssec:alg_qualities}

\input{figures/coreset_size_on_sens_quality.tex}

\paragraph*{Comparison with standard sensitivity sampling.}

We first compare the Fast-Coreset algorithm with standard sensitivity sampling in terms of both quality and runtime.  To this end,
Figure~\ref{fig:coreset_size_on_sens_quality} shows that, across datasets, the Fast-Coreset method produces coresets of consistent quality. Specifically, we
vary the $m$-scalar parameter and see that all distortion values are below $1.2$. Furthermore, choosing a sufficient coreset size ($m$-scalar$=80$) gives
distortion below $1.03$ for both Fast-Coresets and traditional sensitivity sampling. Despite this, we see in Figure~\ref{fig:k_on_runtime} that varying $k$ from
$50$ to $200$ causes a linear slowdown in sensitivity sampling but only a logarithmic one for the Fast-Coreset method.

We take a moment to discuss the Fast-Coreset algorithm's results on the particularly challenging benchmark and geometric datasets.  First, consider that these
datasets are specifically constructed to be difficult to compress, as the sensitivities are awkward to approximate and the clusters have unequal numbers of points.
Furthermore, the nature of the constructions leads to highly varying densities across the dataset, imposing an extra challenge on the HST metric-embeddings.
Despite these obstacles, we see that the Fast-Coreset method manages to find a low-distortion compression for reasonable coreset sizes ($m$-scalar$=80$).

This analysis confirms the theory predicted by \cref{thm:main}: sensitivity sampling coresets can be
implemented without a linear dependency on $k$ while still preserving the coreset property. Given this context, we will not add traditional sensitivity sampling
to further experiments, as it is too slow to run on our large datasets and does not provide better quality results than the Fast-Coreset method.

\paragraph*{Comparison with other coreset methods.}
We now refer the reader to Figure~\ref{fig:coreset_size_on_quality}, where we show the effect of coreset size on the distortion across datasets and methods by
sweeping over $m$-scalar values in $[20, 40, 60, 80]$. 
First, we note that BICO performs poorly on all datasets -- even worse than uniform sampling.
Second, uniform sampling fails catastrophically on the artifical datasets while providing reasonable quality coresets on the better-distributed real datasets.
This is likely due to the real datasets being reasonably well distributed, with consistently sized clusters and without extreme outliers.
Lastly, we see that, as expected, coresets of larger size obtain lower distortion across datasets.
\input{figures/k_on_runtime.tex}

We now comment on the more involved coreset constructions, lightweight and Cut-$k$-means. The Cut-$k$-means coreset surprisingly does not perform better than
lightweight coresets. Our tentative explanation for this lies in the reason why lightweight performs well on some dataset: those are rather uniform instances, well
spread around their means, with clusters of equivalent sizes. \david{not sure it is true} Sampling according to distance to the mean makes therefore sense,
while sampling according to a $j$-means solution (for small $j$) actually introduces bias against points close to the centers of that solution.  Furthermore,
lightweight coreset performs bad when there are $k$ unbalanced clusters, as illustrated in the gaussian-mixture dataset, as in
Figure~\ref{fig:lightweight_breaks}.  Our artificial dataset emphasize this effect, which does not appear in the real-world dataset (probably due to our choice
of large $k$) but is very plausible.  \david{add precise comments, depending on the actual precision of the coreset, especially for lightweight. }
\andrew{I'm not sure what to do with this paragraph. On the one hand, I agree that we should discuss it. On the other hand, I am wary of giving handwavy
explanations without evidence to back them up. Chris, what do you think?}
\david{Agreed on the handwaving -- although interpreting experiments is always somewhat handwavy I guess. Chris?}

\paragraph*{Additional evaluation on imbalanced clusters.}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lcccc}
        Algorithm & $\gamma = 0$ & $\gamma = 1$ & $\gamma = 3$ & $\gamma = 5$\\
        \hline
        LW Coreset & 1.03 & 1.03 & 1.36 & 2.17\\
        $j=2$ & 1.04 & 1.04 & 1.04 & 1.92\\
        $j=\log k$ & 1.04 & 1.04 & 1.04 & 1.95\\
        $j=\sqrt{k}$ & 1.05 & 1.06 & 1.04 & 1.18\\
        Fast Coreset & 1.03 & 1.03 & 1.04 & 1.12
    \end{tabular}
    \caption{The effect of $\gamma$ in the Gaussian mixture dataset on the coreset distortion. We report the means over 5 random dataset generations.
    Each generation had 50\,000 points in 50 dimensions, with 50 Gaussian clusters and coresets of size 4\,000. We set $k=100$.}
        \label{tbl:class-imbalance}
\end{table}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{images/effect_of_gamma.png}
    \caption{Histograms of cluster sizes for different values of $\gamma$ in the Gaussian mixture dataset.}
        \label{fig:effect-of-gamma}
\end{figure}

\david{Say what is the coreset size. Do you know why is Fast Coreset so bad as well? Would it be possible to increase it to have a good fast coreset?}
\andrew{Are you asking why the fast coreset performs poorly in table~\ref{tbl:class-imbalance}? I think it does okay\ldots -- I can mention that this is for
$m=20k$, which is a small coreset. I used a small one to emphasize the difference but can use a bigger one so that our scores aren't as bad.}
\david{Ok, I didn't know the size was so small -- Yes, maybe a bigger one would be great (to be able to claim that fast coreset is consistently below 1.05 or something}

As discussed, the sensitivities for lightweight coresets are obtained by a linear combination of a uniform distribution and each point's relative
distance to the mean. Since the Gaussian mixture dataset has clusters of varying sizes, a small cluster that is close to the mean is unlikely
to ever be sampled. We argue that, although this is a toy dataset, one can easily imagine real-world datasets that have this property. 
As a simple solution, we see that the Cut-$k$-means coreset obtains satisfactory solutions on the Gaussian mixture dataset for 
\david{I don't know what you meant unfortunately}
\andrew{I have not gotten to changing this paragraph, but it will basically say that the table of class imbalance shows that, as class imbalance grows, it is
better to have $j>1$. I intend to add other values of $\gamma$ to that table.}

A similar argument can be made for the more-challenging geometric dataset, where the cluster size decreases exponentially and all clusters are equidistant.
We show furthermore that, for small values of $j$, sensitivities obtained according to
solutions for $j$-means are insufficient to create a coreset for the geometric-progression dataset.

To further measure the effect that the class imbalance has on the quality of each coreset, we define a class imbalance parameter $\gamma$ and obtain each
cluster's size by $|C_i| = \frac{n}{k} \exp \left( \gamma(\rho_i - \frac{1}{2}) \right)$, where $k$ is the number of Gaussian clusters and the $\rho_i$ values
are distributed uniformly at random in the range $[0, 1]$.  We obtain each cluster iteratively: once we have sampled one cluster of size $|C_0|$, we repeat the
process for the next cluster with $|C_1| = \frac{n - |C_0|}{k-1}\exp \left( \gamma(\rho_1 - \frac{1}{2}) \right)$.  Thus, each cluster has size $\frac{n}{k}$
when $\gamma = 0$ and the cluster sizes diverge as $\gamma$ grows. 
\input{figures/lightweight_breaks.tex}

We see the effect of $\gamma$ on the coreset distortion in Table~\ref{tbl:class-imbalance}, where even
small values of $\gamma$ can break the lightweight coreset construction.   Looking at the sensitivities computed via $j$-means++, we see that using $(j>1)$ is better at maintaining the coreset
property for higher values of $\gamma$ but is still not guaranteed to obtain satisfactory compression. Despite this, sensitivity sampling consistently obtains
coresets with low distortion.

\david{the sentence "cluster sizes diverge exponentially as $\gamma$ grows linearly"
is not super clear to me, because of the randomness. Also, since you are only showing two values of $\gamma$, it is hard to say that "it grows linearly": would
it be possible to add the ratio max size / min size for $\gamma = 5$ ? This should be something close to $\exp(10)$ which is crazy huge compared to $50000$, so
maybe not the right thing to plot...}
\andrew{If I'm understanding correctly, you're saying that we should add a value for the largest cluster size divided by the smallest cluster size? I'm happy to
do that. I think we could also just say what it is in expectation.}

