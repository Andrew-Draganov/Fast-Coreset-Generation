\section{Introduction}

The modern data analyst has no shortage of clustering algorithms to choose from but, given the ever-increasing size of relevant datasets, many are often
too slow to be practically useful. 
Indeed, Llyod's method \cite{Lloyd82}, the arguably most popular used clustering algorithm, runs for multiple iterations until convergence with every iteration requiring $O(ndk)$ time, where $n$ is the number of points, $d$ is the number of features and $k$ is the number of clusters. This is prohibitive whenever the data set is very large and has prompted the rise of big data algorithms which provide both theoretical guarantees and
practical improvements for standard data-science techniques. The perspectives
of theoretical soundness and practical efficacy can, however, sometimes be at odds with one another. On the one hand, theoretical guarantees provide assurance that
the algorithm will obtain valid solutions without unlucky inputs affecting their running time or accuracy. On the other hand, it is sometimes difficult to convince
oneself to implement the theoretically optimal algorithm when there are cruder methods that are faster to get running and perform well in practice.

Since datasets can be large in the number of points $n$ and/or the number of features $d$, big-data methods must mitigate the effects of both.
With respect to the feature space, the question is effectively closed as random projections are fast (running in effectively linear time), practical to
implement \cite{makarychev2019performance}, and provide tight guarantees on the embedding's size and quality. The outlook is less clear when reducing the number of points $n$, and there are
two separate paradigms that each provide distinct advantages. 
On the one hand, we have uniform sampling, which runs in sublinear time but may miss important subsets of
the data and therefore can only guarantee accuracy when making assumptions on the data \cite{HuangJL23}.  On the other hand, the most accurate sampling strategies are those that provide the \emph{strong coreset}
guarantee, wherein the cost of any solution on the compressed data is within an $\varepsilon$-factor of that solution's cost on the original dataset \cite{stoc21}.
Surprisingly, many coreset constructions can remove the dependency on $n$, significantly accelerating downstream tasks.

We study both paradigms with respect to a classic problem -- what are the limits and possibilities of compression for the $k$-means and $k$-median objectives?
Whereas uniform sampling provides optimal speed but no worst case accuracy guarantee, all claimed coreset constructions have a running time of at least
$\tilde{O}(nd+nk)$ when yielding tight bounds on the minimum number of samples required for accurate compression. 

It is easy to show that any algorithm that achieves a guarantee must read the entire dataset\footnote{A simple example is to consider a data matrix in
$\mathbb{R}^{n \times d}$ with one single large entry and the remaining entries being very small. The algorithm must find this entry to yield a good
2-clustering.}. Thus a clear open question is what guarantees are achievable in linear or nearly linear time. Indeed, currently available fast sampling
algorithms for clustering \cite{BachemL018} \cite{kmeans_sublinear_bachem16} cannot achieve strong coreset guarantees.  Recently, \cite{DSWY22} proposed
a sensitivity-based method for strong coresets that runs in time $\tilde{O}(nd + nk)$ and conjectured this to be optimal for $k$-median and $k$-means.  Since
the issue of determining a coreset size of optimal size has recently been closed \cite{CSS21,CLSSS22,huangLB}, this is arguably the main open problem in coreset
research for center-based clustering. We resolve this by showing that there exists an easy-to-implement algorithm that constructs coresets in $\tilde{O}(nd)$
time -- only logarithmic factors away from the time it takes to read in the dataset.

Nonetheless, this does not fully illuminate the landscape among sampling algorithms for clustering in practice. Although our algorithm achieves an optimal
runtime and an optimal compression, it is certainly possible that other, cruder methods may be just as viable on most real world data sets despite having no
worst case guarantees.  Thus, there is a natural spectrum within linear-time algorithms where obtaining more information about the dataset allows for a better
sampling strategy at the expense of sublinear factors in speed. We state this formally in the following question: When are optimal $k$-means and $k$-median
coresets necessary and what is the practical tradeoff between coreset speed and accuracy? To this end, we veify that, while sublinear-time methods like uniform
sampling are sufficiently accurate on many real-world datasets, there exist data distributions that cause catastrophic failure. Indeed, these cases can only be
avoided with a strong-coreset method like our proposed algorithm. Thus, while many practical settings do not require the full coreset guarantee, one cannot cut
corners if one wants to be confident in their compression. We verify that this extends to the streaming paradigm and applies to downstream clustering
approaches.

In summary, our contributions are as follows:
\begin{itemize}

    \item We show that one can obtain strong coresets for $k$-means and $k$-median in $\tilde{O}(nd)$ time. This resolves a conjecture on the necessary runtime
        for $k$-means coresets \cite{DSWY22} and is theoretically optimal up to log-factors.

    \item Through a comprehensive analysis across datasets, tasks, and streaming/non-streaming paradigms, we verify that there is a necessary tradeoff between
        speed and accuracy among the linear- and sublinear-time sampling methods. This gives the clustering practitioner a blueprint on when to use each
        compression algorithm for effective clustering results in the fastest possible time.

\end{itemize}
