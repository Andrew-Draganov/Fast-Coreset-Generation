\section{Fast-Coresets}
\label{sec:theory_0}

\begin{algorithm}[tb]
   \caption{Fast-Coreset($P, k, \eps, m$)}
   \label{alg:main}
\begin{algorithmic}[1]
   \State {\bfseries Input:} data $P$, number of clusters $k$, precision $\eps$ and target size $m$
   \State Use a Johnson-Lindenstrauss embedding to embed $\tilde P$ of $P$ into $\tilde d = O(\log k)$ dimensions
   \State Find approx. solution $\tilde \calC = \lbra \tilde c_1, ..., \tilde c_k\rbra $ on $\tilde P$ and assignment $\tilde \sigma : \tilde P \rightarrow
   \tilde \calC$ by \fkmeans.	
   \State Let $\calC_i = \tilde \sigma^{-1}(c_i)$. Compute the $1$-median (or $1$-mean) $c_i$ of each $\calC_i$ in $\R^d$.%, and define $\sigma(p) := c_i$ for all $p \in \calC_i$.
   \State For each point $p \in \calC_i$, define
   $s(p) = \frac{\dist^z(p, c_i)}{\cost(\calC_i, c_i)}+ \frac{1}{|\calC_i|}$.
   \State Compute a set $\coreset$ of $m$ points randomly sampled from $P$ proportionate to $s$.
   \State For each $\calC_i$, define $|\hat \calC_i|$ the estimated weight of $\calC_i$ by $\coreset$, namely $|\hat \calC_i| := \sum_{p \in \calC_i \cap
   \coreset} \frac{\sum_{p' \in P}s(p')}{s(p)m}$.
   \State {\bfseries Output:} the coreset $\coreset$, with weights $w(p) = \frac{\sum_{p' \in P}s(p')}{s(p)m} \lpar (1+\eps)|\calC_i| - |\hat \calC_i|\rpar$
\end{algorithmic}
\end{algorithm}


In this section, we first combine two existing results to produce a strong coreset in time $\tilde{O}(nd \log \Delta)$, where $\Delta$ is the spread of
the input.  We show afterwards how to reduce the dependency in $\Delta$ to $\log \log \Delta$, giving the desired nearly-linear runtime.

Our method is based on the following observations about the group sampling \cite{stoc21} and sensitivity sampling \cite{FeldmanL11} coreset construction
algorithms. Both start by computing a solution $\calC$. When $\calC$ is a $c$-approximation, they compute a $c \eps$-coreset of size $\tilde O\lpar
k \eps^{-z-2}\rpar$ and $\tilde O\lpar k \eps^{-2z -2}\rpar$, respectively. Hence, by rescaling, they provide an $\eps$-coreset with size $\tilde O\lpar
k (\eps/c)^{-z-2}\rpar$ and $\tilde O\lpar k (\eps/c)^{-2z-2}\rpar$.  This leads to the following fact:

\begin{fact}\label{fact:logApprox}[See Theorem 1 in \cite{stoc21} and 4.8 in \cite{FeldmanL11}\footnote{Those references use actually an $O(1)$-approximation,
    but all proofs go through with a  $O\lpar \log^{O(1)} k\rpar$ approximation, with an additional factor  $O\lpar \log^{O(1)} k\rpar$ in the coreset size.}]
    Let $\calC$ be an $O\lpar \log^{O(1)} k\rpar$ approximation to $k$-median or $k$-means.  Then, group sampling using solution $\calC$ computes a coreset of
    size $\tilde O\lpar k \eps^{-z-2}\rpar$, and sensitivity sampling one of size $\tilde O\lpar k \eps^{-2z-2}\rpar$.  Both run in time $\tilde O(nd)$,
    provided $\calC$.  \end{fact}

To turn \cref{fact:logApprox} into an algorithm, we use the quadtree-based \fkmeans approximation algorithm from \cite{cohen2020fast}, which has two key
properties: 
\begin{enumerate}
\item \fkmeans runs in $\tilde O\lpar n d \log \Delta\rpar$ time (Corollary 4.3 in \cite{cohen2020fast}), and
\item \fkmeans computes an assignment from input points to centers that is an $O\lpar d^z \log k\rpar$ approximation to $k$-median ($z=1$) and $k$-means ($z=2$)
    (see Lemma 3.1 in \cite{cohen2020fast} for $z=2$ and the discussion above that lemma for $z=1$). Applying dimension reduction techniques \cite{MakarychevMR19}, the dimension
    $d$ may be replaced by a $\log k$ in time $\tilde O(nd)$. This results in a $O\lpar\log^{z+1} k\rpar$ approximation. 
\end{enumerate}

The second property is crucial for us: the algorithm does not only compute centers, but also assignments in $\tilde{O}(nd\log \Delta)$ time.  We describe how to
combine it with sensitivity sampling in \cref{alg:main} and prove in \cref{ssec:cor_proof} that this computes an $\eps$-coreset in time $\tilde O(nd \log
\Delta)$:

\begin{corollary}\label{cor:mainAlg}
\cref{alg:main} runs in time $\widetilde O\lpar n d \log \Delta\rpar$ and computes an $\eps$-coreset for $k$-means.
\end{corollary}
Furthermore, we generalize \cref{alg:main} to other fast $k$-median approaches in \cref{app:extensions}.

Thus, one can combine existing results to obtain an $\eps$-coreset without an $\widetilde{O}(nk)$ time-dependency.  However, this has only replaced the
$\widetilde{O}(nd + nk)$ runtime by $\widetilde{O}(nd \log \Delta)$. Indeed, the spirit of the issue remains -- this is not near-linear in the input size.

We verify this by devising a dataset that has $n - n'$ points uniformly in the $[-1, 1]^2$ square. Then, for $r \in \mathbb{Z}^+$, we produce a sequence of
points at $(0, 1), (0, 0.5), \cdots, (0, 0.5^r)$ and copy this sequence $n' / r$ times, each time with a different $x$ coordinate. The result is a dataset of
size $n$ where $\log \Delta$ grow linearly with $r \in o(n)$. The resulting linear time-dependency can then be seen in Table \ref{tbl:logdelta}.

\input{tables/log_delta_runtime.tex}

\section{Reducing the Impact of the Spread}
\label{sec:theory} %\label{sec:logdelta}
\newcommand{\boxsize}{\textsc{MaxDist}}
We now show how to replace the linear time-dependency on $\log \Delta$ with a logarithmic one.

\paragraph*{Overview of the Approach}

WLOG, we assume that the smallest pairwise distance is at least $1$, and $\Delta$ is a (known) upper-bound on the diameter of the input. To remove
the $\log\Delta$ dependency, we proceed by producing a substitute dataset $P'$ that has a lower-bound on its minimum distance and an upper-bound on its maximum
distance. We then show that, with overwhelming likelihood, reasonable solutions on $P'$ have the same cost as solutions on $P$ up to an additive error.

In order to produce the substitute dataset $P'$, we first find a crude upper-bound on the cost of the optimal solution to the clustering task on $P$. We
then create a grid such that, for every cluster in the optimal solution, the cluster is entirely contained in a grid cell with overwhelming likelihood -- in some sense, points in different grid cell do not interact. We can then freely move these
grid cells around without affecting the cost of the solution, as long as they stay far enough away so that points in different cells still do not interact. 
Thus, we can constrain the diameter of $P'$ to be small with respect to the quality of the approximate solution. We show later how to constrain the minimum distance to be comparable to the quality of this approximate solution as well.

% More formally, if $U$ is a $c$-approximation of the optimal cost then the minimum distance can be reduced by rounding all coordinates to multiples of $g = U/(cn)$,
% giving us a minimum distance of no less than $g$; it is then enough to reduce the diameter to $\poly(n) U$.  For this, we place a grid with cell length $O(n
% \cdot U)$ centered at a random location, so that two points from the same cluster in $\opt$ fall into the same cell with high probability. This implies that
% distinct cells do not interact with each other in any reasonable solution.

We will focus this section on the simpler $k$-median problem but show how to reduce $k$-means to this case in \cref{app:redKM}.

\subsection{Computing a crude upper-bound}
\label{ssec:crude_bound}

As described, we start by computing an approximate solution $U$ such that $\opt \leq U \leq \poly(n) \cdot \opt$. For this, the first step is to embed the input
into a quadtree. This embedding has two key properties: first, distances are preserved up to a multiplicative factor $O(d \log \Delta)$, and therefore the
$k$-median cost is preserved up to this factor as well. Second, the metric is a \emph{hierarchically separated tree}: it can be represented with a tree, where
points of $P$ are the leafs, and the distance between two points is given by the depth of their lowest common ancestor: if it is at depth $\ell$, their distance
is $\sqrt{d} \Delta 2^{-\ell}$.  Our first lemma shows that finding the first level of the tree for which the input lies in $k+1$ disjoint subtrees provides us
with the desired approximation. 

\begin{lemma}\label{lem:apxTree} [Proof in \cref{app:apx-tree-proof}]
Let $\ell$ be the first level of the tree with at least $k+1$ non-empty subtrees. Then, $\sqrt{d}2^{-\ell+1} \cdot \Delta \leq
\opt_T \leq n \cdot \sqrt{d}2^{-\ell+4} \cdot \Delta$, where $\opt_T$ is the optimal $k$-median solution in the tree metric.

\end{lemma}

We prove this in \cref{app:apx-tree-proof}. A direct consequence  is that the first level of the tree for which at least $k+1$ cells are non empty allows us to
compute an $O(n)$-approximation for $k$-median on the tree metric. Since the tree metric approximates the oringial Euclidean metric up to $O(d \log
\Delta)$, this is therefore an $O(n d \log \Delta)$-approximation to $k$-median in the Euclidean space.

To turn this observation into an algorithm, one needs to count the number of non-empty cells at a given level $\ell$: for each point, we identify the cell that
contains it using modulo operations. Furthermore, we count the number of distinct non-empty cells using a standard dictionary data structure. This is done in
time $\tilde O(nd)$, and pseudo-code is given \cref{alg:crudeApx}. Using a binary search on the $O(\log \Delta)$ many levels then gives the following result:

\begin{lemma}\label{lem:crudeApx}[Proof in \cref{app:apx-tree-proof}]
There is an algorithm running in time $\tilde O(nd \log \log \Delta)$ that computes an $O(n d \log \Delta)$-approximation to $k$-median, and $O(n^3 d^2 \log^2
\Delta)$-approximation to $k$-means.
\end{lemma}

Given this crudez approximate solution, it remains to create a substitute dataset $P'$  that
satisfies two requirements:
\begin{enumerate}
    \item First, $P'$ must have spread linear in the quality of the approximate solution. If this holds, Algorithm~\ref{alg:main} on $P'$ will take
        $\tilde{O}(nd \log \log \Delta)$ time.
    \item Second, any reasonable solution on $P'$ should be roughly equivalent to a corresponding solution on $P$. This would mean that running
        Algorithm~\ref{alg:main} on $P'$ gives us a valid coreset for $P$.
\end{enumerate}

\subsection{From Approximate Solution to Reduced Spread}
\label{ssec:reduce_spread}

Let $U$ be an upper-bound on the optimal cost, computed via \cref{lem:crudeApx}. We place a grid with side length $r:= \sqrt{d} n^2\cdot U$, centered at
a random point in $\{0, ..., r\}^d$.  The following lemma ensures that with high probability (over the randomness of the center of the grid), no cluster of the
optimal solution is spread on several grid cells.

\begin{lemma}\label{lem:quadtreeSep}

The probability over the center's location that two points $p$ and $q$ are in different grid cells is at most $\frac{\|p-q\|}{n^2 U}$

\end{lemma}
\begin{proof}
We first bound the probability that there is a grid line along the $i$-th dimension between $p$ and $q$. Let $p_i, q_i$ be the $i$-th coordinate of $p$ and $q$,
assume $p_i \leq q_i$ and let $\ell \in \mathbb{Z}$ such that $p_i - \ell r \in [0, r)$.  Then, $p$ and $q$ are separated along dimension $i$ if and only if the
$i$-th coordinate of the center is in $[p_i - \ell r, q_i - \ell r]$. This happens with probability $|p_i - q_i|/r$. 

Therefore, a union-bound over all coordinates shows that $p$ and $q$ are in different grid cells with probability at most $\sum |p_i-q_i| / r \leq \sqrt{d} \|p-q\|/r$.
\end{proof}

Since $U$ is larger than the distance between any input point and its center in the optimal solution, a union-bound ensures that with probability $1-1/n$, no
cluster of this solution is split among different cells.  In particular, there are at most $k$-non empty cells which we can identify using a dictionary.  We
call these ``boxes".% and associate the $j$-th box with the $j$-th cluster in our solution from Lemma~\ref{lem:crudeApx}, for $1 \leq j \leq k$. 
Each box $j$ has a middle point, which we call $m_j$.

From this input, we build a new set of points $P'$ as follows.  For each dimension $i \in \lbra 1, ..., d \rbra$, sort the $k$ boxes according to their value on
dimension $i$. Then, for each $j \in \lbra 1, ..., k\rbra$, let $m^i_j$  be the $i$-th coordinate of the middle-point of the $j$-th box. If $m^i_{j+1} - m^i_j
\geq 2r$, then for all boxes $j'$ with $j' > j$, shift the points of $j'$ by $m^i_{j+1} - m^i_j - 2r$ in the $i$-th dimension. This can be implemented in
near-linear time, as described in \cref{alg:reduce-diam} (presented in \cref{app:pseudoCode}). In essence, we take boxes that are far apart and bring them
closer together. The dataset $P'$ obtained after these transformations has the following properties:

% From this input, we build a new set of points $P'$ as follows: first, identify the non empty cell (using a dictionary as previously). We associate each box
% with its middle point.  For each coordinate $i \in \lbra 1, ..., d \rbra$, sort the $k$ centers according to their value on coordinate $i$. Then, for each $j
% \in \lbra 1, ..., k\rbra$, let $c^i_j$  be the $i$-th coordinate of centers of the $j$-th box. If $c^i_{j+1} - c^i_j \geq 2r$, then for all boxes $j'$ with $j'
% > j$, shift the points of $j'$ by $c^i_{j+1} - c^i_j - 2r$ in the $i$-th coordinate. This can be implemented in near-linear time, as described in
% \cref{alg:reduce-diam} (presented in \cref{app:pseudoCode}). The dataset $P'$ obtained after these transformations has the following properties:

\begin{proposition}\label{prop:boxes}
    Let $P'$ be the dataset produced by Algorithm~\ref{alg:reduce-diam}. It holds that:
    \begin{enumerate}
        \item in $P'$, the diameter of the input is $O(d n^2\cdot U \cdot k)$ with probability $1-1/n$ over the randomness of the grid, and
        \item two boxes that are adjacent (respectively non-adjacent) in $P$ are still adjacent (resp. non-adjacent) in $P'$.
    \end{enumerate}
\end{proposition}
\begin{proof}

By construction, the maximal distance between the middle-points of any two boxes in $P'$ is $2r = 2\sqrt d n^2\cdot U$. \cref{lem:}, ensures that, with probability $1-1/n^2$, any two points in the same cluster of the optimal solution (e.g., at distance less than $\opt \leq U$ from each other) are in the same box. Therefore, a union-bound ensures that there are at most $k$ boxes.

Therefore, the total distance along a coordinate is at most $2kr$, and the diameter of the whole point set is $\sqrt{d} \cdot 2kr$.

If two boxes were adjacent in $P$, then along any dimension their middle-points have distance at most $r$. Thus, if one of the adjacent boxes moves along any dimension, the other must as well and they will remain
adjacent.
If they are not adjacent, there is at least one dimension where their middle-points are at distance at least $2r$ from one another. Along each such dimension,
their shift cannot bring them closer than to within $2r$ of one another and they will stay non-adjacent.

\end{proof}

The first property allows us to reduce the spread to $(nd \log \Delta)^{O(1)}$.  Indeed, one can round all coordinates of points in $P'$ to the closest multiple
of $g := \frac{U}{n^4 d^{2} \log \Delta}$.  Combined with the diameter reduction, this ensures that the spread of the dataset obtained is at most $(nd \log
\Delta)^{O(1)}$.  Furthermore, the second property of \cref{prop:boxes} combined with the choice of $g$ ensures that the cost of any reasonable solution is the
same before and after the transformation, as stated in the following lemma:

\begin{lemma}\label{lem:reduceSpread}

Let $P'$ be the outcome of the diameter reduction and rounding steps. With probability $1-1/n$ over the randomness of the grid, $P'$ has spread $(nd \log
\Delta)^{O(1)}$.

Suppose $U$ is such that $\opt \leq U \leq d n^2 \opt$, and let $\calS'$ be a solution for $k$-median (resp. $k$-means) on $P'$ with cost at most $d n^2
\opt$ (resp. $d^2 n^4 \opt$ for $k$-means). Then, one can compute a $k$-median solution for $P$, with the same cost as $\calS'$ for $P'$ up to an
additive error $\opt / n$, in time $O(nd)$. The same holds by reversing the roles of $P$ and $P'$.

\end{lemma}
\begin{proof}
First, in rounding points to the closest multiple of $g$, the distance between any point and its rounding is at most $g \leq \frac{U}{n^4 d^2 \log \Delta} \leq
\frac{\opt}{n^2}$. Summing over all points, any solution computed on the gridded data has cost within an additive factor $\pm \frac{\opt}{n}$ of the true cost. 

% Now, consider that two points at distance more than $d n^2\cdot U$ are not in the same cluster of $\calS'$: otherwise, the cost of $\calS'$ would be larger than $d n^2\cdot U \geq d n^2\cdot \opt$. 

Let $\calS$ be the solution obtained from $\calS'$ by reversing the construction of $P'$, namely re-adding the shift that was substracted to every box. Since
adjacency is preserved by Proposition~\ref{prop:boxes}, all points that are in the same cluster have the same shift, and therefore all intra-cluster distances
are the same in $P$ and $P'$.  Therefore, the costs are equal and, by extension, $\cost \left( \calS \right) \in \cost \left( \calS' \right) \pm \opt / n$,
where the additive $\opt /n$ comes from the rounding.

Finally, the smallest non-zero distance is $g = \frac{U}{n^4 d^{2} \log \Delta}$, and with probability $1-1/n$ the diameter is $\sqrt{d} \cdot 3d n^2\cdot
U \cdot k$ (see \cref{prop:boxes}), implying that the spread of $P'$ is $(nd \log \Delta)^{O(1)}$.
\end{proof}

Combining the algorithm of \cref{lem:crudeApx}, which gives a bound on $U$, with \cref{lem:reduceSpread} brings us to the following theorem:

\begin{theorem}

Given $P \subset \R^d$ with spread $\Delta$, there is an algorithm running in time $O(nd \log \log \Delta)$ that computes a set $P'$ such that (1) with
probability $1-1/n$, the spread of $P'$ is $\poly(n,d, \log(\Delta))$ and (2) any solution with cost at most $d n^2 \opt$ for  $k$-median (resp. $d^2 n^4 \opt$
for $k$-means) on $P'$ can be converted in time $O(nd)$ into a solution with same cost on $P$, up to an additive error $O(\opt / n)$.

\end{theorem}

To summarize, we have shown that one can, in $\tilde{O}(nd)$ time, find a modified dataset $P'$ that preserves the cost of corresponding $k$-means and
$k$-median solutions from $P$. Importantly, this $P'$ has spread that is logarithmic in the spread of $P$. As a result, one can apply Algorithm \ref{alg:main}
onto $P'$ in $\tilde{O}(nd)$ time without compromising the compression quality with respect to $P$. Lastly, this compression on $P'$ can be re-formatted onto
$P$ in $\tilde{O}(nd)$ time.
