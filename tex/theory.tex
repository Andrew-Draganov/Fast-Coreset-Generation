\section{The algorithm, and proof of \cref{thm:main}}

In this section, we present our algorithm and a very clean proof of \cref{thm:main}. Those are based on the following observations about the group
sampling \cite{stoc21} and sensitivity sampling \cite{FeldmanL11} coreset construction algorithms.

Both start by computing a solution $\calC$. When $\calC$ is a $c$-approximation, the two algorithms compute a $c \eps$-coreset of size $\tilde O\lpar
k \eps^{-z-2}\rpar$ and $\tilde O\lpar k \eps^{-2z -2}\rpar$, respectively. Hence, by rescaling, they provide $\eps$-coreset with size $\tilde O\lpar
k (\eps/c)^{-z-2}\rpar$ and $\tilde O\lpar k (\eps/c)^{-2z-2}\rpar$. 

Therefore, taking $c = O(\log k)$ merely affects the size of the computed coreset, while allowing for approximation algorithms that are way faster.

Due to the difficulty of verifying coresets, it is important to have a theoretical proof of efficiency of the coreset algorithms.  Showing, as we do, that one can use $O(\log k)$-approximations (e.g., $k$-means++) while benefiting from the same
guarantees as with a slow $O(1)$-approximation is thus not only an artifact, but a crucial piece of coreset's success. 

It is indeed tempting to use those large-factor approximations, especially since they seem to perform better in practice. 
It might however have been the case that coresets constructed that way have poor quality, without being able to notice it due to the aforementioned hardness. 
We show that this unfortunate event does not happen, and that, even with the ``worst-case" guarantee $O(\log k)$, these algorithms indeed produce coresets.

Our algorithm is based on the \fkmeans algorithm from \cite{cohen2020fast}: it allows to embed points in a very simple metric,  where it is easy to compute the
distance to any given set of centers. The two key properties of this algorithm are the following: it runs in $\tilde O\lpar n d \log \Delta\rpar$ time, and
computes an assignment from input point to centers that is a $O\lpar d^z \log k\rpar = O\lpar\log^{z+1} k\rpar$ approximation to $k$-median ($z=1$) and
$k$-means ($z=2$).  We use it, in combination with sensitivity sampling, as described in \cref{alg:main}.


\begin{algorithm}[tb]
   \caption{Fast-Coreset}
   \label{alg:main}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $P$, number of clusters $k$, precision $\eps$ and target size $S$
   \STATE Use Johnson-Lindenstrauss embedding to reduce the dimension to $O(\log k)$.
   \STATE Compute an approximate solution $\calC$ and an assignment $\sigma : P \rightarrow \calA$ using \fkmeans.
   \STATE For each point $p \in P$, let $\calC_p := \sigma^{-1}(\sigma(p))$ be the points in the same cluster as $p$, and define 
   $s(p) = \frac{\dist^z(p, \sigma(p))}{\sum_{p' \in \calC_p} \dist^z(p', \sigma(p'))} + \frac{1}{|\calC_p|}.$
   \STATE {\bfseries Output:} a set $\coreset$ of $S$ points randomly sampled from $P$ proportionate to $s$.
\end{algorithmic}
\end{algorithm}


We formalize the proof of \cref{thm:main}, using the previous algorithm.

\begin{proof}[Proof of \cref{thm:main}]
Performing the Johnson-Lindenstrauss embedding takes time $\tilde O(nd)$, and allows to consider $\tilde d=O(\log k)$ with only a constant-factor loss in the approximation quality  \cite{makarychev2019performance}. 
On the projected dataset, the algorithm \fkmeans runs in time $\tilde O\lpar n \log \Delta\rpar$, and the solution it computes has cost $O\lpar d^z \log k\rpar = O\lpar\log^{z+1} k\rpar$. 
\chris{I think we should mention why we can get the sensitivity scores. Given any $\log k$ approximation, this step is still not trivial.}

Furthermore, the values $s(p)$ defined in \cref{alg:main} can be used to perform the sensitivity sampling algorithm: \cite{FeldmanL11} showed that sampling according to those yield a coreset with size $\tilde O \lpar k d \eps^{-2z}\rpar$, improved by \cite{HuangV20} to $\tilde O \lpar k \eps^{-2z -2} \rpar$.
\end{proof}

\david{I removed the two-step algorithm, as I think we are not experimenting it}
%
%\subsection{A Two-Step Algorithm}
%We present a different algorithm, based on the following standard transitivity property: if $\coreset_1$ is an $\eps$-coreset of $P$, and $\coreset_2$ an $\eps$-coreset of $\coreset_1$, then $\coreset_2$ is a $2\eps$-coreset of $P$.
%
%This property could be used as follows: computing a precise approximation for $P$ may be slow. Instead, one could spend the time budget as follows:
%
%\begin{enumerate}
%\item First, compute a rough approximation and use it to build an $\eps/2$-coreset $\coreset_1$. Due to the poor quality of approximation, this coreset may be larger than the target size; but still way smaller than $P$.
%\item  Then, compute a precise approximation on $\coreset_1$, and use it to build a smaller $\eps/2$-coreset $\coreset_2$ of $\coreset_1$. 
%\end{enumerate}
%By transitivity, $\coreset_2$ is therefore a small $\eps$-coreset for $P$. The bottleneck running time, to compute the precise approximation, now depends merely on $|\coreset_1|$ instead of $P$: one can therefore hope to be able to compute a much more precise estimations of the sensitivities required to sample a small coreset afterwards.
%
%The theoretical result does not improve compared to \cref{thm:main}, but this algorithm may be more practical. Therefore, we try it as well in our experiments.
%
%\david{add something depending on the results of experiments}

\subsection*{Extensions.} 
We conclude with a few remarks, that allow to generalize \cref{alg:main}.
First, instead of using sensitivity sampling, one could use the group sampling procedure of \cite{stoc21}, or virtually any coreset construction: all those run in linear time, once provided with an initial solution $\calC$ and assignment $\sigma$. 
Therefore, our algorithm could use as a subroutine any coreset construction algorithm, if one is developed that improves over sensitivity and group sampling.

Second, one could compute the solution $\calC$ via a different algorithm: \cref{alg:main} only need to be provided an assignment to a solution that is a $O(\polylog k)$ approximation. 
This initial solution may as well be an $O\lpar\polylog \lnor P \rnor_0 \rpar$ approximation: using the iterative coreset construction from \cite{BravermanJKW21}, one could then derive a near-optimal coreset size, only suffering a $O(\log^* n)$ loss in the running time.

As an example, we illustrate a different approach for $k$-median. One could first embed the input into a hierarchically separated tree (HST) with expected distortion $O(\log \lnor P \rnor_0)$ \cite{FakcharoenpholRT03}: on such tree metrics, solving $k$-median can be done in linear time, using dedicated algorithms (see e.g. \cite{Cohen-AddadLNSS21}). Using the solution from the HST metric, one can compute a coreset, and iterate using the previous argument.
This embedding into HST is very similar to what is done by the \fkmeans algorithm, but can be actually performed in \emph{any} metric space, not only Euclidean. 
As an example, in a metric described by a graph with $m$ edges, the running time of this construction would be near linear-time $\tilde O(m)$.
