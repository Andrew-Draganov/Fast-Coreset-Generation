\section{Introduction}

In the age of ``big data", storing datasets with hundreds of billions of points can render even seemingly efficient algorithms intractible. Thus, it has become
increasingly necessary to devise compression methods that are both theoretically sound and practically effective across diverse sets of data. The perspectives
of theoretical soundness and practical efficacy are, however, slightly at odds with one another. On the one hand, theoretical guarantees provide assurance that
the algorithm will obtain valid solutions without unlucky inputs affecting their runtime or accuracy. On the other hand, it is sometimes difficult to convince
oneself to implement the theoretically optimal algorithm when there are cruder methods that are faster to get running and perform well in practice.

In this light, we consider both the theoretical and experimental limits of compression under the $k$-median and $k$-means tasks by studying coresets\ldots %FIXME

From the theoretical perspective, [reference] conjectured that an $O(nd + nk)$ running time is necessary to obtain $k$-median and $k$-means coresets with all of
the corresponding guarantees. It is clear that $O(nd)$ time is necessary, as one must read the full dataset before making any claims about one's performance on
it.  Thus, the question is whether there exists a linear-time coreset construction that preserves correctness across the entire space of datasets and solutions.
We first respond to the conjecture by showing that there is a way to make $k$-clustering coresets that remove the $O(nk)$ time dependency.  However, this in
itself is not sufficient to guarantee linear time as it relies on subdividing the space into a tree to avoid the dependency on $k$ and, subsequently, the
maximum depth of this tree depends linearly on the bit-precision of the input. That is, since the tree has a depth equal to the ratio of the largest value over
the smallest value in the input, any algorithm that builds the full tree now has a linear time-dependency on $\log \Delta$, where $\Delta$ is the maximum
such ratio attainable. To this end, we show that even this linear-time dependency can be avoided by doing x, y, z. Thus, we affirm that there is an
$\tilde{O}(nd)$ time algorithm for making $k$-means and $k$-median coresets. We refer to this method as Fast-Coresets.

Despite the guarantees offered by the above algorithm, it is certainly possible that another, faster algorithm can obtain effective coresets by compromising on
edge cases. We evaluate the validity of this hypothetical method by practically analyzing the full spectrum of relevant coreset runtimes.  Naturally, uniform
sampling is the fastest method as it does not even require one to process the entire input. Since this clearly breaks on datasets with outliers, any algorithm
that beats uniform sampling must run in $O(nd)$ time. Indeed, the popular Lightweight-Coresets method samples the coreset based on the $1$-means approximation
-- the simplest approximation to the $k$-means task that exists in $O(nd)$ time. On the other hand, the Fast-Coreset algorithm acts as a natural upper bound as
it runs in $\tilde{O}(nd)$ time and provides full theoretical guarantees due to the $O(1)$ approximation it computes. Thus, we are interested in the tradeoff
between the quality of the $k$-means approximation and the coreset's validity.

With respect to this tradeoff, we show that an $O(1)$ approximation is indeed necessary if one wants to be confident in their compression, as even simple
counterexamples can break the coresets sampled on suboptimal approximations to the objective. However, we preempt the practitioner's concerns, as we also report
the full spectrum of results to illustrate where each construction does and does not work. For example, it is indeed the case that improving the approximation
quality improves the coreset accuracy, even on the counterexample datasets. Furthermore, we perform extensive experiments on real-world data to impress upon the
reader that our counterexamples are hand-crafted -- it is often not necessary to obtain the full $O(1)$ approximation so long as your data is reasonably
well-behaved. Specifically, the rule of thumb we find is that datasets with strong outliers or uneven densities are likely to thwart $k$-centering coreset
constructions that cut even small corners.

Thus, we show that the $\tilde{O}(nd)$ runtime and the $O(1)$ approximation factor are tight both in theory and in practice when paired with sensitivity
sampling. Said otherwise, there exists an easy-to-implement algorithm that finds $k$-median and $k$-means coresets in $\tilde{O}(nd)$ time and any coreset
algorithm that utilizes a worse approximation factor is easily thwarted by counterexamples. Despite this, we also show the full spectrum of compression results
in case one is interested in balancing speed and efficacy. The remaining structure of our paper is as follows:

