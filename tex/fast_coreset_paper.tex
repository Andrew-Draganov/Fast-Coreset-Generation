\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{changepage}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[backend=bibtex]{biblatex}

\DeclareMathOperator*{\argmin}{arg\,min}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\bibliography{references}
\allowdisplaybreaks

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\title{Fast Coreset Generation for K-Means}
\author{Andrew Draganov}

\begin{document}
\maketitle

\section{Introduction}

As the reliance on large datasets grows, the need for effective compression algorithms grows as well. To this end, \emph{coresets} have become popular as
a method to reduce the number of points in a dataset while preserving all of its characteristics. Specifically, given a pointset $P \in \mathbb{R}^{n \times D}$
a solution space $\mathcal{C}$, and a loss function $\mathcal{L}(P, \mathcal{C})$, a coreset $Q$ of $S \ll n$ points has the property that $\mathcal{L}(Q,
\mathcal{C}_i) \in (1 \pm \epsilon) \mathcal{L}(P, \mathcal{C}_i)$, where $\mathcal{C}_i$ is any point in the solution space. Importantly, for many problems the
coreset size $S$ does not depend on the number of input points $n$. 

Since one of the purposes of coresets is to accelerate downstream tasks, one also requires that the coreset is constructed as efficiently as possible.  In this
paper we study acceleration techniques for $k$-means coresets. Recent work has shown that these can be obtained in $O(nd + nk)$ time, where $d$ is the
dimensionality after a Johnson-Lindenstrauss (JL) transform. Their algorithm consists of three main steps on the JL transformed input.

First, one produces an $O(1)$ approximation of the solution in $\tilde{O}(nd)$ time. Second, this approximation is used to upper-bound
the sensitivity values . The last step, then, is to sample $S$ points proportional to their upper-bounded sensitivities, where $S = \tilde{O}(k^2 d)$
guarantees the coreset property.

The computational bottleneck here is the second step -- estimating the sensitivities. It is not known how to compute them exactly, so the available solution is
to obtain an upper-bound using all $n \times k$ point-center assignments in the approximate solution. Importantly, once one has the clustering matrix the
sensitivities are available in no linear time.

The key insight, then, is that the point-center assignments can be obtained in faster than $O(nk)$ time. Specifically, one can use Hierarchically-Separated
Trees (HSTs) to simplify the approximation. HSTs impose a distance metric $d_{tree}$ such that $d(x, y) \leq d_{tree}(x, y) \leq \alpha d(x, y)$, where $\alpha
\in O(d^2)$ for the $k$-means problem. This requires $\tilde{O}(nd)$ time, which is an unavoidable expense during coreset construction as it is the time needed
to read the dataset.  Importantly, however, we show that one can obtain a perfect $k$-means clustering on the tree in $O(n)$ time. This means that the
cluster-assignment burden has been reduced from $O(nk)$ to $\tilde{O}(nd)$ time at the expense of an $\alpha \in O(\log n)$ distortion.

The remaining issue is that the size of our coreset, $S$, now contains an extra dependency on $\alpha \in O(\log d^2)$. To this end, we point out that we can
iterate on this coreset by producing a new coreset $\bar{Q}$ for it. Importantly, $\bar{Q}$'s construction will \emph{only} depend on the parameters $d$, $k$,
and $\varepsilon$, as $S = |Q|$ does not depend on $n$. Thus, the final coreset can be constructed from $Q$ with no time dependency on $n$ and is absorbed into
the existing complexity of obtaining $Q$.

\section{Related Work}

\section{Preliminaries}
\subsection{Sensitivity}

Modern coreset construction methods rely on estimating the sensitivity $\sigma_p = \max_{C \in \mathcal{C}} \frac{\text{cost}(p, C)}{\text{cost}(P, C)}$ of each
point in the dataset. Given the sensitivities, one simply samples $S$ points from $P$ proportional to $\sigma$, where $S$ depends on $k, D, \varepsilon,
\delta$. Since it is not known how to obtain the sensitivities directly, it suffices to sample according to the upper-bound $\hat{\sigma}_p = \text{Cost}(p,
\mathcal{C}_i) / \left( \sum_{q \in \mathcal{C}_i}\text{Cost}(q, \mathcal{C}_i) \right) + 1 / |\mathcal{C}_i|$, where $\mathcal{C}_i$ is the center that point
$p$ belongs to. This requires obtaining both an approximate solution $\mathcal{C}$ and all $n \times k$ point-center assignments, imposing a bottleneck on the
coreset construction time complexity.

The other consequence of the approximation is that we must oversample the coreset to account for the error that the approximation introduced. Specifically, if
we have an $(\alpha, \beta)$-approximation of the optimal solution, our number of samples $S$ must scale linearly with $\alpha$ in order to satisfy the coreset
requirements.

\subsection{Tree Embeddings}

Tree embeddings have proven to be very effective for accelerating clustering tasks. They rely on the premise that a space can be recursively subdivided such
that, with appropriately chosen edge weights, the distance between two points in the original space is upper-bounded by their distance in the tree. In this
paper we use randomly-shifted binary kd-trees. We start by shifting all points by a vector $s \in [0, \text{MaxDist}]^d$, where $\text{MaxDist}$ is the largest
difference in $P$ along any one dimension. Thus, all points in a single cell of diameter $2\sqrt{d}\text{MaxDist}$ with weight to its children equal to its
diameter.

We then subdivide the points one dimension at a time such that after $d$ splits, the diameter of the cell is $\frac{1}{2}$ that of its $d$-th grandparent. This
results in a tree with one point in each leaf and all leaves at equal depth. Then the distance between two points in the tree is the sum of cell diameters along
the path between the two corresponding leaves. It was shown in \textbf{ref needed} that this produces a tree embedding such that $\text{dist}(x, y) \leq
\mathbb{E}[\text{dist}_{tree}(x, y)] \leq 24 \log(n) \text{dist}(x, y)$, implying a distortion of at most $O(log(n))$ on the original distances.

\subsection{Tree Clustering}

One benefit of working with trees is that they admit an optimal $k$-means and $k$-median clustering on the tree metric. This is achieved through a dynamic
program that maintains the solutions of each cell's subtrees, providing an inductive proof that optimality is preserved under appropriate merges. Although tree
clustering requires $O(ndk^2)$ time, it can be parallelized to run in $O(n\log n)$ time by exploiting the fact that the output of a cell depends on the outputs
of its children. Thus, one can obtain a $k$-median solution by embedding $P$ into an HST and obtaining the optimal clustering on the tree metric, incurring 
$O(log(n))$ error on the optimal $k$-median solution for $P$.

This proves trickier for $k$-means as the HST distortion bound is \emph{not} guaranteed on the squared distances, implying that the incurred error does not
scale with the distance distortion. To this end, \textbf{ref needed} showed that one can instead fit $3$ HST's and compare the squared Euclidean distances to
the squared minimum distance over the trees. Under this setting, one has that $\text{dist}^2(x, y) \leq \text{min\_tree\_dist}^2(x, y) \leq d^2 \text{dist}^2(x,
y)$.


\section{Algorithm Description}

\subsection{$\tilde{O}(nD + nk)$ time coreset generation}

\begin{algorithm}
\caption{$\tilde{O}(nd + nk)$-time coreset construction}
\label{naive_coreset_alg}
\hspace*{\algorithmicindent} \textbf{Input} \\
\hspace*{\algorithmicindent*2} $P$ : Pointset to be compressed. \\
\hspace*{\algorithmicindent*2} $k$ : Number of centers for downstream clustering. \\
\hspace*{\algorithmicindent*2} $\varepsilon$, $\delta$ : Approximation constants. \\
\hspace*{\algorithmicindent} \textbf{Output} \\
\hspace*{\algorithmicindent*2} $Q$ : Coreset for $P$ with $|Q| \ll |P|$
\begin{algorithmic}[1]

    \item[]
    \Function{Approx Sensitivities}{$P, \mathcal{C}$}
    \State $\hat{\sigma} \gets [\; ]$
    \For{$0 \leq i < |P|$}
    \State $\mathcal{C}_i \gets $ Center that $P[i]$ belongs to \Comment{Takes $O(kd)$ time if not precomputed}
    \State $\hat{\sigma}[i] \gets \frac{\text{Cost}(P[i], \mathcal{C}_i)}{\sum_{q \in \mathcal{C}_i}\text{Cost}(q, \mathcal{C}_i)} + \frac{1}{|\mathcal{C}_i|}$
    \EndFor
    \State \Return $\hat{\sigma}$
    \EndFunction
    \item[]

    \Function{Construct Optimal Coreset}{$P, k, \varepsilon, \delta$}
    \State $P \gets \text{JLTransform}(P, O(\log k))$
    \State $\mathcal{C} \gets O(1)$-approximation of OPT on $P$
    \State $\hat{\sigma} \gets $ Approx Sensitivities$(P, \mathcal{C}$)
    \State $Q \gets $ Sample Points$(P$, weights $=\hat{\sigma}$, NumSamples $=\frac{dk^2}{\varepsilon} \log ^2 \frac{kd}{\delta}$)
    \State \Return $Q$
    \EndFunction
    
\end{algorithmic}
\end{algorithm}

Before discussing our algorithm for fast coreset generation, we first explain where the existing method, Algorithm \ref{naive_coreset_alg}, can be improved
upon. 

There are two bottlenecks in Algorithm \ref{naive_coreset_alg}. First, we require a solution that provides an $O(1)$ approximation of the cost. Second, we must
obtain all $n \times k$ point-center assignments so that we can upper-bound the sensitivities. We point out, however, that these are not equivalent tasks.  To
see this, let $\mathcal{C} = \{ c_1, \dots, c_k \} $ be an O(1) approximation obtained by $k$-means++. Since $k$-means++ does not explicitly
store the cost of every point with respect to its closest center, would need to still perform all $n \times k$ point-to-center comparisons to find the
cluster assignments. On the other hand, if we have all the assignments then we can quickly get the cost of all the centers (and the sensitivities) in $O(n)$
time. Thus, accelerating algorithm \ref{naive_coreset_alg} can be accomplished by obtaining the assignments in faster than $O(nk)$ time.

\subsection{Identifying clusters quickly}

Our goal, then, is to find an approximation algorithm that will \emph{simultaneously} identify the cluster centers and the cluster assignments in faster than
$O(nk)$ time. On an intuitive level, this means that we want to exploit the following likely condition: if point $p$ is far from center $i$ and center $i$ is
close to center $j$, then we can reasonably conclude that $p$ is also far from center $j$.  We point out, however, that a generalization of this is exactly the
reasoning underpinning tree-embeddings -- distances are bounded by the dimensions of the subspaces points belongs to.

Furthermore, HSTs are easy to work with as they can be constructed quickly in $O(nd \log n \log(d \Delta))$\footnote{I think?\dots}, where $\Delta$ is the
maximum ratio between distances in $P$, and can be clustered similarly quickly in $O(n \log n)$ time \footnote{Again, I'm not sure on the exact bound here}.

Given this context, our first algorithm can be found in Algorithm \ref{fast_coreset_alg}. We first project the data down to $O(\log k)$ dimensions using a JL
transform. Next, we embed the dataset into an HST and find the cluster centers on the tree. Due to the tree's structure, knowing the centers leads to having the
point-cluster assignments in linear time, as we can traverse the tree from left to right and identify each point with one of its neighboring centers. We can now
obtain a coreset $Q$ by sampling $P$ according to the sensitivities bounded by these clusters. However, since our centers are based on an $O( \log n)$
distortion of the distances, we must oversample by the errors incurred during the approximations in order to satisfy the coreset property.

\subsection{Obtaining the optimal coreset}

Although the algorithm described above produces a coreset $Q$ in near-linear time, it contains a factor of $\log (n)$ unnecessary points in order to account for
the approximation errors. However, note that we now have a dataset $Q$ whose size does not depend at all on $n$. Thus, we are now free to utilize $Q$ to obtain
an optimal coreset in any manner we choose, as we can be confident that this will take negligible time compared to the complexity of producing $Q$ in the first
place. If it turns out that $|Q|$ is still too big, our above fast HST algorithm can be repeated multiple times, where $i$ iterations will produce a coreset
with $\log( \dots \log(n) \dots )$ unnecessary points, where the expression has $i$ nested logarithms. Putting all of this together, we present our full
algorithm in \ref{full_coreset_alg}.

\algdef{SE}% flags used internally to indicate we're defining a new block statement
[STRUCT]% new block type, not to be confused with loops or if-statements
{Class}% "\Struct{name}" will indicate the start of the struct declaration
{EndClass}% "\EndStruct" ends the block indent
[1]% There is one argument, which is the name of the data structure
{\textbf{class} \textsc{#1}}% typesetting of the start of a struct
{\textbf{end class}}% typesetting the end of the struct

\begin{algorithm}
\caption{$\tilde{O}(nd)$-time coreset construction}
\label{fast_coreset_alg}
\begin{algorithmic}[1]

    \item[]
    \Class{HST}
        \State \textbf{variables} $\gets P$
    \EndClass

    \item[]
    \Function{fit}{root$, \text{split\_dim}=0, \Delta=0,  \text{split\_location} = [0, \dots, 0]$}
    \If{ $\text{len}(\text{root}.P) \leq 1$ }
        \State \Return
    \EndIf
    \State $\mathcal{I}_l \gets \left( node.P[:, \text{split\_dim}] \leq (\text{split\_location}[\text{split\_dim}] + \Delta) \right)$
    \State $S_l \gets \text{root}.P[\mathcal{I}_l]$
    \State $S_r \gets \text{root}.P[\lnot \; \mathcal{I}_l]$
    \item[]
    \State // Add $1$ to dim or wrap around to 0. Divide $\Delta$ by 2 when wrap around.
    \State split\_dim, $\Delta$ $\gets$ NextSplit(split\_dim, $\Delta$) 
    \State FIT(HST($S_l$, split\_dim, split\_location, $\Delta$)
    \State split\_location[split\_dim] $+= \Delta$
    \State FIT(HST($S_r$, split\_dim, split\_location, $\Delta$)
    \EndFunction
    \item[]

    \Function{Cluster HST}{root$, k$}
    \State Do $O(n\log n)$-time optimal clustering on HST
    \EndFunction
    \item[]

    \Function{Construct Fast Coreset}{$P, k, \varepsilon, \delta$}
    \State $P \gets \text{JL Transform}(P, O(\log k))$
    \State root $ \gets$ HST($P$)
    \State FIT(root)
    \State $\tilde{\mathcal{C}} \gets $ Cluster HST(root, $k$)
    \State $\hat{\sigma} \gets $ Approx Sensitivities$(P, \tilde{\mathcal{C}}$)
    \State $Q \gets $ Sample Points$(P$, weights $=\hat{\sigma}$, NumSamples $=\frac{dk^2}{\varepsilon} \log ^2 \frac{kd}{\delta}$)
    \State \Return $Q$
\EndFunction
    
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{$\tilde{O}(nd)$-time optimal coreset construction}
\label{full_coreset_alg}
\begin{algorithmic}[1]
    \Function{Construct Fast Optimal Coreset}{$P, k, S, \varepsilon, \delta$}
    \State $\tilde{Q} \gets P$
    \While{ $|\tilde{Q}| > S$ }
        \State $\tilde{Q} \gets$ Construct Fast Coreset($\tilde{Q}, k, \varepsilon, \delta$)
    \EndWhile
    \State $Q \gets$ Construct Optimal Coreset($\tilde{Q}, k, \varepsilon, \delta$)
    \State \Return $Q$
    \EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{k-median clustering in $O(n)$ time on a 2-RHST}

\begin{algorithm}
\caption{$O(n)$-time algorithm for $k$-median in a $2$-RHST}\label{RHST_alg}
\hspace*{\algorithmicindent} \textbf{Input} \\
\hspace*{\algorithmicindent*2} Root : Top-level node of tree. \\
\hspace*{\algorithmicindent*2} Dists : Array of $n-1$ distances from leaf $i$ to leaf $i+1$. Obtained during HST creation.
\begin{algorithmic}[1]

\Function{2-RHST $k$-Median}{Root, Dists, $k$}
    \BState \emph{Base Case}:
    \State // Make a center for each leaf
    \If {$\text{Root } \textbf{is} \text{ BaseNode}$} \Comment A BaseNode is any node with $<k$ leaves
        \State $\mathcal{C} \gets \text{Array}( \; \text{Center}(l, [l], l, l) \textbf{ for } l \textbf{ in } \text{Root.leaves}\;)$
            \State \Return $\mathcal{C}$
        \EndIf
    \BState \emph{Recursive Call}:
    \State // Combine two sets of centers together
    \State $\mathcal{C}_L \gets$ 2-RHST $k$-Median(Root.LeftChild, Dists, $k$)
    \State $\mathcal{C}_R \gets$ 2-RHST $k$-Median(Root.RightChild, Dists, $k$)
    \State MinCenterDists $\gets$ FindMinDists($\mathcal{C}_L$, $\mathcal{C}_R$, $k$)
    \State $\mathcal{C} \gets $ Array( )
    \For {$0 \leq i < k$}
        \State MinDist $\gets$ MinCenterDists[$i$]
        \State $\mathcal{C}$[$i$] $\gets$ Merge(MinDist.LeftCenter, MinDist.RightCenter)
    \EndFor
    \State \Return $\mathcal{C}$
\EndFunction
\item[]

\Class{Center}
    \State \textbf{variables} $\gets$ Loc, PointList, RightMostPoint
\EndClass
\item[]

\Function{Merge}{$c_1$, $c_2$}
    % FIXME -- is this true?
    \State Points $ \gets c_1\text{.PointList} \cup c_2\text{.PointList}$
    \If {$c_1\text{.size} \neq c_2\text{.size}$}
    \State Loc $ \gets (\text{argmax}(|c_1|, |c_2|)$).Loc \Comment Center will be the center of the larger one
    \State \Return Center(Loc, Points, $c_2$.RightMostPoint)
    \EndIf
    \State \Return Center($c_1$.Loc, Points, $c_2$.RightMostPoint) \Comment WLOG
\EndFunction
\item[]

\Function{FindMinDists}{$\mathcal{C}_L$, $\mathcal{C}_R$, $k$, Dists}
    \State // Get $k$ smallest weighted distances between centers
    \State $\mathcal{C} \gets \mathcal{C}_L \cup \mathcal{C}_R$
    \State WeightedDists $\gets$ Array( )
    \For{$0 \leq i < 2k - 1$}
        \State D $\gets$ Dists$ [\mathcal{C}_i\text{.RightMostPoint}]$ \Comment ``Distance from this leaf to the one on its right''
        \State Size $\gets$ $\min(|\mathcal{C}_i|, |\mathcal{C}_{i+1}|)$ \Comment ``How many points will pay the cost''
        \State WeightedDists[$i$] $\gets$ D $\times$ Size
    \EndFor
    \State MinCenterDists $\gets$ minimum $k$ values from WeightedDists (unsorted) \Comment Requires $O(k)$ time
    \State \Return MinCenterDists
\EndFunction

\end{algorithmic}
\end{algorithm}
% 
% It appears to be possible to get an optimal k-median clustering in a 2-RHST in $O(n)$ time. The underlying reason for this is that each center $c_i$
% has one closest center $c_j$ and, for all leaves $v \in c_i$, $v$'s second closest center is $c_j$. Thus, we can recursively find the $k$-median clustering
% quickly since the merges only depend on positions of the centers, not the points associated to them.
% 
% The time complexity comes from the fact that we will recurse down until the base case of $k$ leaves in a cell. There are $O(n/k)$ of these. We now go up the
% recursive stack by merging the centers of subtrees together. Since every center's closest center is either directly to its left or its right, we only need to
% perform $2k$ comparisons to find the $k$ closest centers. Since we are do not need these to be sorted, finding the $k$ smallest elements in a list takes $O(k)$
% time. Since we start with $O(n/k)$ separate subtrees and each merge reduces the number of subtrees by one, we will perform $O(n/k)$ merges where each merge
% requires $O(k)$ time. Thus, our runtime is $O(n)$.
% 
% We now present the algorithm for clustering a $2$-RHST in $O(n)$ time. We say that centers can be placed on any node in the tree, not just on leaves.
% 
% Assume that we have created a binary tree embedding where all leaves are at the same height and each edge to a node's child is half the weight of the
% edge to that node's parent. Assume also that during the creation of the embedding, we stored a list of nodes with fewer than $k$ leaves. We define this
% list as $A$ such that $|a| \leq k \; \forall \; a \in A$, where the cardinality of a node is the number of leaves it has under it. Call these the
% \emph{base nodes}.
% 
% Furthermore, for each leaf $l$ in base node $a$, we store the tree-distance to the leaf on its left and the leaf on its right in $a$.
% Thus, for every base node, we have its list of leaves as well as how far each leaf is to its left and right neighbors. These can all be obtained
% during the creation of the HST in no additional time. We now describe the recursive step.
% 
% We first consider the base case. If our current root node $a$ is a base node, then we assign one center to each leaf in the
% base node and store the distance of each center to its left and right neighbors. If $a$ has fewer than $k$ leaves, then this just means there are unassigned
% centers. Notice that this is inherently an optimal clustering.
% 
% If we are not in the base case, we can assume that we have left and right children, each with $k$ centers\footnote{Assume that neither child has
% unassigned centers. The algorithm won't change if a child has an unassigned center, it just requires fewer merges.} Our algorithm will proceed by
% merging the $2k$ centers until we have $k$ of them. Theorem \ref{optimality-preservation} shows that these merges preserve optimality.
% 
% We define the merge function $M : v \times v \rightarrow v$ such that $M(c_i, c_j)$ is the node that minimizes the sum of distances to all the leaves in
% $c_i$ and $c_j$. The merges take place in the order of smallest incurred cost over all the leaves connected to the merge. We now look through the $2k$
% centers to see which ones should be merged together\footnote{assuming that the previous centers were optimal for their respective subtrees}.
% Importantly, the $2$-RHST structure guarantees that each merge will occur between neighboring left-right centers.
% 
% \begin{theorem}
%     Let node $N$ in a $2$-RHST have left and right children with optimal $k$-median clusterings. Then the optimal merge is $M(c_i, c_{i+1})$ for $0 \leq i < 2k$.
% \end{theorem}
% \begin{proof}
%     For the sake of contradiction, assume that $M(c, b)$ has minimum cost where $b$ is not $c$'s closest center. WLOG, let $M(c, b)$ be closer to $b$
%     than to $c$, implying that $|b| > |c|$. Lastly, let $\hat{c}$ be $c$'s closest center.
% 
%     There exists a node where the paths $c \rightarrow o$ and $c \rightarrow \hat{c}$ diverge from each other. By the $2$-RHST configuration, the edge above
%     this fork has larger weight than the entire path from the fork to any leaf below it. We know that $\hat{c}$ is below this fork.
% 
%     Clearly, then, it is cheaper for $c$'s leaves to be assigned to $M(c, \hat{c})$ than to $M(c, b)$. This reduces our cost by at least $|c| \cdot
%     d\left(M(c, b), M(c, \hat{c})\right)$.
% 
%     The remaining question is whether the added cost of connecting $\hat{c}$'s leaves to $M(c, \hat{c})$ can be paid for.  If
%     $|\hat{c}| > |c|$, then $M(c, \hat{c}) = \hat{c}$ and the added cost from $\hat{c}$'s leaves is 0. On the other hand, if $|\hat{c}| \leq |c|$, then we
%     know that $|\hat{c}| < |b|$, since $M(c, b)$ was closer to $b$ than to $c$. Then the cost of connecting $\hat{c}$'s leaves to $M(c, \hat{c})$ is equal
%     to
% 
%     \[ |\hat{c}| \cdot d(\hat{c}, M(c, \hat{c})) \leq |c| \cdot d(\hat{c}, M(c, \hat{c})) \leq |c| \cdot d\left(M(c, b), M(c, \hat{c})\right) \]
% 
%     Thus, it will never be cheaper to connect $c$ to any center other than $\hat{c}$.
% \end{proof}
% 
% We have now shown that the optimal merge occurs among closest centers. However, it is not yet clear that performing the merges will give the optimal
% solution. This turns out to be the case.
% 
% \begin{theorem}
%     \label{optimality-preservation}
% 
%     Let node $N$ in a $2$-RHST have left and right children with optimal $k$-median clusterings. Then performing $k$ minimum-cost merges produces the optimal
%     $k$-median clustering on $N$.
% 
% \end{theorem}
% \begin{proof}
% 
%     Let $\mathcal{C}_L$ and $\mathcal{C}_R$ be the optimal $k$-median centers on $N$'s left and right children, respectively. Let $\mathcal{C}$ be the result of
%     performing $k$ merges and $OPT$ be the optimal $k$ centers on $N$. If we assume that $\mathcal{C} \neq OPT$, then there exists a leaf $v$ whose cost under
%     $\mathcal{C}$ is greater than its cost under $OPT$. WLOG, let $v$ belong to center $\mathcal{C}_L^i$. We can assume that that $\mathcal{C}_L^i$ was involved
%     in a merge as, if it wasn't, $v$ would belong to its previous optimal clustering and have cost less than or equal to its cost under $OPT$.
% 
%     Let $\mathcal{C}_{opt}$ be the center that $v$ is assigned to in $OPT$. It must then be the case that $\mathcal{C}_{opt}$ is higher than
%     $\mathcal{C}_L^i$ but lower than $M(\mathcal{C}_L^i, \mathcal{C}_L^j)$, where $\mathcal{C}_L^j$ is the center that $\mathcal{C}_L^i$ merges with.  We
%     know that $\mathcal{C}_{opt}$ must cover additional leaves compared to $\mathcal{C}_L^i$. If it didn't, it would not be optimal for it to be above
%     $\mathcal{C}_L^i$. Let $v_j \in \mathcal{C}_{opt} ; v_j \notin \mathcal{C}_L^i$ be one such leaf. We know that $v_j \notin \mathcal{C}_L^j$ since, if it
%     were, merging $\mathcal{C}_L^i$ and $\mathcal{C}_L^j$ would be optimal. We also know that $v_j$'s lowest common ancestor with $\mathcal{C}_L^i$ is
%     below $\mathcal{C}_L^j$. Thus, we must conclude that $v_j$ is closest to either $\mathcal{C}_L^i$ or $\mathcal{C}_L^j$ but did not belong to either of
%     them in the optimal clustering, providing us a contradiction.
% 
% \end{proof}

% \section{Questions}
% \begin{itemize}
%     \item In \cite{cohen2022scalable}, it is stated on page 10 that ``two points $p$ and $q$ have probability $\frac{d \cdot \text{dist}(p, q)}{2^i}$ to be cut at
%         level $i$. Thus, the expected distance squared between $p$ and $q$ is $d \cdot \text{dist}(p, q) \cdot \sum_i \sqrt{d}2^i$ which means that the squared
%         distance can be distorted by an arbitrarily large factor in expectation. Where does the expected distance squared come from?
%     \item The k-median dynamic program works on the assumption that we have binary trees. This is done by splitting along one dimension at a time. Does the same
%         process work for the k-means tree embedding without losing any quality? Seems like it should\dots
% \end{itemize}

\printbibliography
\end{document}
